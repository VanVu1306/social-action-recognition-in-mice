{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9657fc78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:36.561013Z",
     "iopub.status.busy": "2025-12-09T07:14:36.560307Z",
     "iopub.status.idle": "2025-12-09T07:14:48.229390Z",
     "shell.execute_reply": "2025-12-09T07:14:48.228534Z"
    },
    "papermill": {
     "duration": 11.676308,
     "end_time": "2025-12-09T07:14:48.230634",
     "exception": false,
     "start_time": "2025-12-09T07:14:36.554326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HARDWARE CHECK\n",
      "============================================================\n",
      "GPU Available: True\n",
      "GPU Name: Tesla T4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json, ast\n",
    "from sklearn.base import clone, ClassifierMixin, BaseEstimator\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os, math\n",
    "import gc\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Hardware check\n",
    "import torch\n",
    "print(\"=\"*60)\n",
    "print(\"HARDWARE CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843c27da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.240782Z",
     "iopub.status.busy": "2025-12-09T07:14:48.240235Z",
     "iopub.status.idle": "2025-12-09T07:14:48.251008Z",
     "shell.execute_reply": "2025-12-09T07:14:48.250276Z"
    },
    "papermill": {
     "duration": 0.017027,
     "end_time": "2025-12-09T07:14:48.252183",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.235156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # mode = \"validate\"\n",
    "    mode = \"tune\"\n",
    "    tune_thresholds_mode = True\n",
    "\n",
    "    model_save_dir = \"/kaggle/working\"\n",
    "    model_tune_dir = \"/kaggle/input/ensemble-mabe-model-result\"\n",
    "    # model_save_dir = \"D:/UET/ML/mouse_behavior/social-action-recognition-in-mice\"\n",
    "\n",
    "    train_csv_path = \"/kaggle/input/MABe-mouse-behavior-detection/train.csv\" \n",
    "    test_csv_path = \"/kaggle/input/MABe-mouse-behavior-detection/test.csv\"\n",
    "    train_annotation_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\"\n",
    "    train_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\"\n",
    "    test_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/test_tracking\"\n",
    "\n",
    "    # train_csv_path = \"D:/UET/ML/mouse_behavior/data/train.csv\" \n",
    "    # test_csv_path = \"D:/UET/ML/mouse_behavior/data/test.csv\"\n",
    "    # train_annotation_path = \"D:/UET/ML/mouse_behavior/data/train_annotation\"\n",
    "    # train_tracking_path = \"D:/UET/ML/mouse_behavior/data/train_tracking\"\n",
    "    # test_tracking_path = \"D:/UET/ML/mouse_behavior/data/test_tracking\"\n",
    "\n",
    "    drop_body_parts =  [\n",
    "        'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "        'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "        'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n",
    "    ]\n",
    "\n",
    "    # Threshold range: typically 0.20-0.40, with 0.27 as a good starting point\n",
    "    # Higher thresholds = fewer false positives, more false negatives\n",
    "    # Lower thresholds = more false positives, fewer false negatives\n",
    "    action_thresholds = {\n",
    "        \"default\": 0.27,           # Global fallback threshold\n",
    "        \"single_default\": 0.26,    # Default for single mouse behaviors- lowered to improve recall\n",
    "        \"pair_default\": 0.28,      # Default for pair behaviors - slightly higher to reduce false positives\n",
    "        \"single\": {\n",
    "            \"rear\": 0.30,          # Higher threshold - distinctive behavior, reduce false positives\n",
    "            \"groom\": 0.28,         # Slightly higher - common behavior, needs good confidence\n",
    "            \"sniff\": 0.25,         # Lower threshold - subtle behavior, improve recall\n",
    "            \"dig\": 0.29,           # Higher threshold - distinctive behavior\n",
    "            \"eat\": 0.27,           # Standard threshold - balanced precision/recall\n",
    "            \"drink\": 0.27,         # Standard threshold - balanced precision/recall\n",
    "            \"sleep\": 0.24,         # Lower threshold - rare but important, improve recall\n",
    "        },\n",
    "        \"pair\": {\n",
    "            \"attack\": 0.24,        # Lower threshold - rare but critical behavior, maximize recall\n",
    "            \"mount\": 0.28,         # Higher threshold - distinctive behavior, reduce false positives\n",
    "            \"sniff\": 0.26,         # Lower threshold - subtle social behavior, improve recall\n",
    "            \"groom\": 0.27,         # Standard threshold - balanced precision/recall\n",
    "            \"chase\": 0.25,         # Lower threshold - important social behavior, improve recall\n",
    "            \"follow\": 0.26,        # Lower threshold - subtle behavior, improve recall\n",
    "            \"approach\": 0.27,      # Standard threshold - balanced precision/recall\n",
    "        }\n",
    "    }\n",
    "\n",
    "    default_window = [5, 15, 30, 60, 120]\n",
    "\n",
    "    window_map = {\n",
    "        (\"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_tip\"): [5, 15, 30, 45],\n",
    "        (\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_tip\"): [5, 15, 30, 45],\n",
    "        (\"body_center\", \"ear_left\", \"ear_right\", \"nose\", \"tail_base\"): [9, 21, 30, 60, 120],\n",
    "        (\"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"neck\", \"nose\", \"tail_base\"): [5, 9, 15, 30],\n",
    "        (\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\", \"tail_tip\"): [15, 30, 60, 150, 300],\n",
    "        (\"ear_left\", \"ear_right\", \"head\", \"tail_base\"): [5, 15, 30, 60, 120, 240],\n",
    "        (\"ear_left\", \"ear_right\", \"nose\", \"tail_base\", \"tail_tip\"): [15, 30, 60, 120, 360],\n",
    "        (\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\"): [5, 15, 30, 60, 120, 240],\n",
    "        (\"body_center\", \"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\", \"tail_tip\"): [5, 15, 30, 60, 120]\n",
    "    }\n",
    "\n",
    "    default_min_dur = 4\n",
    "\n",
    "    min_duration = {\n",
    "        \"single\": {\n",
    "            \"biteobject\": 23,\n",
    "            \"climb\": 16,\n",
    "            \"dig\": 14,\n",
    "            \"exploreobject\": 6,\n",
    "            \"freeze\": 23,\n",
    "            \"genitalgroom\": 14,\n",
    "            \"huddle\": 10,\n",
    "            \"rear\": 10,\n",
    "            \"rest\": 48,\n",
    "            \"run\": 14,\n",
    "            \"selfgroom\": 10\n",
    "        },\n",
    "        \"pair\": {\n",
    "            \"attack\": 7,   # Attack có thể ngắn\n",
    "            \"'attack'\": 7,\n",
    "            \"dominance\": 14,\n",
    "            \"'dominance'\": 14,\n",
    "            \"sniff\": 5,\n",
    "            \"'sniff'\": 5,\n",
    "            \"allogroom\": 25,\n",
    "            \"approach\": 9,\n",
    "            \"attemptmount\": 10,\n",
    "            \"avoid\": 21,\n",
    "            \"chase\": 9,\n",
    "            \"chaseattack\": 5,\n",
    "            \"defend\": 7,\n",
    "            \"disengage\": 15,\n",
    "            \"dominancegroom\": 17,\n",
    "            \"dominancemount\": 12,\n",
    "            \"ejaculate\": 197,\n",
    "            \"escape\": 9,\n",
    "            \"flinch\": 5,\n",
    "            \"follow\": 9,\n",
    "            \"intromit\": 60,\n",
    "            \"mount\": 12,\n",
    "            \"reciprocalsniff\": 4,\n",
    "            \"shepherd\": 31,\n",
    "            \"sniff\": 5,\n",
    "            \"sniffbody\": 6,\n",
    "            \"sniffface\": 6,\n",
    "            \"sniffgenital\": 6,\n",
    "            \"submit\": 6,\n",
    "            \"tussle\": 12\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47bcec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.260971Z",
     "iopub.status.busy": "2025-12-09T07:14:48.260500Z",
     "iopub.status.idle": "2025-12-09T07:14:48.377504Z",
     "shell.execute_reply": "2025-12-09T07:14:48.376928Z"
    },
    "papermill": {
     "duration": 0.122747,
     "end_time": "2025-12-09T07:14:48.378870",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.256123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(CFG.train_csv_path) \n",
    "test_csv = pd.read_csv(CFG.test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a154dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.388182Z",
     "iopub.status.busy": "2025-12-09T07:14:48.387971Z",
     "iopub.status.idle": "2025-12-09T07:14:48.405551Z",
     "shell.execute_reply": "2025-12-09T07:14:48.405045Z"
    },
    "papermill": {
     "duration": 0.023259,
     "end_time": "2025-12-09T07:14:48.406591",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.383332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_lab = train_csv[\"lab_id\"].str.startswith(\"MABe22\")\n",
    "mask_behavior = train_csv[\"behaviors_labeled\"].isna() | (train_csv[\"behaviors_labeled\"].str.strip() == \"\")\n",
    "mask_drop = mask_lab | mask_behavior\n",
    "\n",
    "train = train_csv[~mask_drop]\n",
    "body_parts_list = list(np.unique(train.body_parts_tracked))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d45036",
   "metadata": {
    "papermill": {
     "duration": 0.003769,
     "end_time": "2025-12-09T07:14:48.414404",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.410635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Functions for Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02dcc18b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.423641Z",
     "iopub.status.busy": "2025-12-09T07:14:48.423456Z",
     "iopub.status.idle": "2025-12-09T07:14:48.438996Z",
     "shell.execute_reply": "2025-12-09T07:14:48.438283Z"
    },
    "papermill": {
     "duration": 0.022105,
     "end_time": "2025-12-09T07:14:48.440257",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.418152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_models(save_dir):\n",
    "    \"\"\"Load all saved models\"\"\"\n",
    "    import joblib\n",
    "    import glob\n",
    "    \n",
    "    all_models = {'single': {}, 'pair': {}}\n",
    "    \n",
    "    for filepath in glob.glob(f\"{save_dir}/models/*.pkl\"):\n",
    "        filename = filepath.split('/')[-1]\n",
    "        mode_type = filename.split('_')[0]\n",
    "        models_dict = joblib.load(filepath)\n",
    "        all_models[mode_type][filepath] = models_dict\n",
    "        print(f\"Loaded {len(models_dict)} models from {filename}\")\n",
    "    \n",
    "    return all_models\n",
    "\n",
    "\n",
    "def run_threshold_tuning_optuna(train_csv, all_models, n_trials=100):\n",
    "    \"\"\"Tune thresholds using Optuna\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTUNA THRESHOLD TUNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Filter data\n",
    "    mask = ~(train_csv[\"lab_id\"].str.startswith(\"MABe22\") | \n",
    "             train_csv[\"behaviors_labeled\"].isna())\n",
    "    train = train_csv[mask]\n",
    "    \n",
    "    # Validation split\n",
    "    videos = train['video_id'].unique()\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(videos)\n",
    "    val_videos = videos[int(len(videos)*0.8):]\n",
    "    val_split = train[train['video_id'].isin(val_videos)]\n",
    "    \n",
    "    print(f\"Using {len(val_videos)} videos for validation\")\n",
    "    \n",
    "    # Collect predictions\n",
    "    val_data = {\n",
    "        'single': defaultdict(lambda: {'y_true': [], 'y_pred_proba': []}),\n",
    "        'pair': defaultdict(lambda: {'y_true': [], 'y_pred_proba': []})\n",
    "    }\n",
    "    \n",
    "    for mode_type in ['single', 'pair']:\n",
    "        for body_parts_str, models_dict in all_models[mode_type].items():\n",
    "            section = body_parts_str.split('_')[1]  # Extract from filename\n",
    "            section = int(section.strip(\".pkl\"))\n",
    "            body_parts_tracked_str = body_parts_list[section]\n",
    "            body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "            if len(body_parts_tracked) > 5:\n",
    "                body_parts_tracked = [b for b in body_parts_tracked if b not in CFG.drop_body_parts]\n",
    "            # Lấy các rows/videos được thu với body_parts_tracked tương ứng\n",
    "            val_subset = val_split[val_split.body_parts_tracked == body_parts_tracked_str]\n",
    "            \n",
    "            if val_subset.empty:\n",
    "                continue\n",
    "            \n",
    "            fps_lookup = val_subset.groupby(\"video_id\")[\"frames_per_second\"].first().to_dict()\n",
    "            \n",
    "            for mode, data, meta, labels in generate_mouse_data(\n",
    "                val_subset, mode=\"train\",\n",
    "                generate_single=(mode_type==\"single\"),\n",
    "                generate_pair=(mode_type==\"pair\")\n",
    "            ):\n",
    "                fps_i = _fps_from_meta(meta, fps_lookup, 30.0)\n",
    "                \n",
    "                if mode_type == \"single\":\n",
    "                    X = build_single_features(data, body_parts_tracked, fps_i)\n",
    "                else:\n",
    "                    X = build_pair_features(data, body_parts_tracked, fps_i)\n",
    "                \n",
    "                X_np = X.to_numpy(np.float32)\n",
    "                \n",
    "                for action, model_list in models_dict.items():\n",
    "                    if action not in labels.columns:\n",
    "                        continue\n",
    "                    \n",
    "                    probs = [m.predict_proba(X_np)[:, 1] for m in model_list]\n",
    "                    val_data[mode_type][action]['y_pred_proba'].extend(np.mean(probs, axis=0))\n",
    "                    val_data[mode_type][action]['y_true'].extend(labels[action].values)\n",
    "    \n",
    "    # Optuna tuning\n",
    "    tuned = {'single': {}, 'pair': {}}\n",
    "    \n",
    "    for mode_type in ['single', 'pair']:\n",
    "        print(f\"\\n{mode_type.upper()} THRESHOLDS:\")\n",
    "        \n",
    "        for action, data in val_data[mode_type].items():\n",
    "            y_true = np.array(data['y_true'])\n",
    "            y_pred = np.array(data['y_pred_proba'])\n",
    "            \n",
    "            if y_true.sum() == 0:\n",
    "                tuned[mode_type][action] = 0.27\n",
    "                continue\n",
    "            \n",
    "            pos_ratio = y_true.sum() / len(y_true)\n",
    "            \n",
    "            def objective(trial):\n",
    "                if pos_ratio < 0.01:\n",
    "                    t = trial.suggest_float('t', 0.15, 0.35)\n",
    "                elif pos_ratio < 0.05:\n",
    "                    t = trial.suggest_float('t', 0.18, 0.38)\n",
    "                else:\n",
    "                    t = trial.suggest_float('t', 0.20, 0.40)\n",
    "                return f1_score(y_true, (y_pred >= t).astype(int), zero_division=0)\n",
    "            \n",
    "            study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "            study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "            \n",
    "            tuned[mode_type][action] = round(study.best_params['t'], 2)\n",
    "            print(f\"  {action:20s} = {tuned[mode_type][action]:.2f} (F1={study.best_value:.3f})\")\n",
    "    \n",
    "    # Print formatted\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COPY TO CFG:\")\n",
    "    print(\"=\"*60)\n",
    "    print('\"action_thresholds\": {')\n",
    "    print('  \"single\": {')\n",
    "    for a, t in sorted(tuned['single'].items()):\n",
    "        print(f'    \"{a}\": {t},')\n",
    "    print('  },')\n",
    "    print('  \"pair\": {')\n",
    "    for a, t in sorted(tuned['pair'].items()):\n",
    "        print(f'    \"{a}\": {t},')\n",
    "    print('  }')\n",
    "    print('}')\n",
    "    \n",
    "    return tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e320a7",
   "metadata": {
    "papermill": {
     "duration": 0.003779,
     "end_time": "2025-12-09T07:14:48.447988",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.444209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9fcc8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.457466Z",
     "iopub.status.busy": "2025-12-09T07:14:48.457288Z",
     "iopub.status.idle": "2025-12-09T07:14:48.471052Z",
     "shell.execute_reply": "2025-12-09T07:14:48.470244Z"
    },
    "papermill": {
     "duration": 0.019905,
     "end_time": "2025-12-09T07:14:48.472182",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.452277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_mouse_data(datasubset, mode, traintest_directory=None, generate_single=True, generate_pair=True):\n",
    "    \"\"\"\n",
    "    Yields:\n",
    "        (mode, X, meta, y)\n",
    "        mode: \"single\" hoặc \"pair\"\n",
    "        X: raw features DataFrame\n",
    "        meta: metadata DataFrame\n",
    "        y: labels (đối với train mode) hoặc action list (đối với test mode)\n",
    "    \"\"\"\n",
    "\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{mode}_tracking\"\n",
    "        # traintest_directory = f\"D:/UET/ML/mouse_behavior/data/{mode}_tracking\"\n",
    "\n",
    "    for idx, row in datasubset.iterrows():\n",
    "        lab_id = row.lab_id\n",
    "        video_id = row.video_id\n",
    "        pix_per_cm = row.pix_per_cm_approx\n",
    "        fps = row.frames_per_second\n",
    "\n",
    "        # Bỏ qua MABe22 labs hoặc missing behaviors\n",
    "        if lab_id.startswith(\"MABe22\"):\n",
    "            continue\n",
    "        if mode == \"train\" and (pd.isna(row.behaviors_labeled) or str(row.behaviors_labeled).strip() == \"\"):\n",
    "            continue\n",
    "\n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "\n",
    "        # Load tracking\n",
    "        vid = pd.read_parquet(path)\n",
    "\n",
    "        # Bỏ bớt bodyparts\n",
    "        if len(np.unique(vid.bodypart)) > 5:\n",
    "            vid = vid[~vid.bodypart.isin(CFG.drop_body_parts)]\n",
    "\n",
    "        pvid = vid.pivot(\n",
    "            index=\"video_frame\",\n",
    "            columns=[\"mouse_id\", \"bodypart\"],\n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n",
    "        pvid /= pix_per_cm\n",
    "\n",
    "        del vid\n",
    "        gc.collect()\n",
    "\n",
    "        mouse_ids = pvid.columns.get_level_values(0).unique().tolist()\n",
    "\n",
    "        # Tìm behaviors tracked trong CSV file\n",
    "        vid_behaviors = json.loads(row.behaviors_labeled)\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=[\"agent\", \"target\", \"action\"])\n",
    "\n",
    "        # Load annotation (đối với training mode)\n",
    "        if mode == \"train\":\n",
    "            try: \n",
    "                annot = pd.read_parquet(path.replace(\"train_tracking\", \"train_annotation\"))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        else:\n",
    "            annot = None\n",
    "\n",
    "\n",
    "        # Build data cho single mouse \n",
    "        if generate_single:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\")\n",
    "\n",
    "            for mouse_id_str in vid_behaviors_subset.agent.unique():\n",
    "                try:\n",
    "                    mouse_id = int(mouse_id_str.replace(\"mouse\", \"\"))\n",
    "                \n",
    "                    if mouse_id not in mouse_ids:\n",
    "                        continue\n",
    "\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n",
    "\n",
    "                    # Single mouse raw features - toạ độ bodyparts\n",
    "                    single_mouse = pvid.loc[:, mouse_id]\n",
    "                    assert len(single_mouse) == len(pvid)\n",
    "                \n",
    "                    # Single mouse meta data\n",
    "                    meta = pd.DataFrame({\n",
    "                        \"video_id\": video_id,\n",
    "                        \"agent_id\": mouse_id_str,\n",
    "                        \"target_id\": \"self\",\n",
    "                        \"video_frame\": single_mouse.index,\n",
    "                        \"frames_per_second\": fps\n",
    "                    })\n",
    "\n",
    "                    # Single mouse labels\n",
    "                    if mode == \"train\":\n",
    "                        labels = pd.DataFrame(0.0, index=single_mouse.index, columns=vid_agent_actions)\n",
    "                    \n",
    "                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n",
    "\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            labels.loc[annot_row[\"start_frame\"]:annot_row[\"stop_frame\"], annot_row.action] = 1.0\n",
    "                        yield \"single\", single_mouse, meta, labels\n",
    "                    else:\n",
    "                        yield \"single\", single_mouse, meta, vid_agent_actions\n",
    "\n",
    "                except (KeyError, ValueError):\n",
    "                    pass\n",
    "\n",
    "        # Build data cho mouse pair\n",
    "        if generate_pair:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n",
    "\n",
    "            if len(vid_behaviors_subset) > 0:\n",
    "                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values(\"mouse_id\")), 2):\n",
    "                    agent_str = f\"mouse{agent}\"\n",
    "                    target_str = f\"mouse{target}\"\n",
    "\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n",
    "\n",
    "                    if len(vid_agent_actions) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Mouse pair raw features - toạ độ bodyparts của cặp chuột\n",
    "                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=[\"A\", \"B\"])  # Raw coordinates\n",
    "                    assert len(mouse_pair) == len(pvid)\n",
    "\n",
    "                    # Mouse pair meta data\n",
    "                    meta = pd.DataFrame({\n",
    "                        \"video_id\": video_id,\n",
    "                        \"agent_id\": agent_str,\n",
    "                        \"target_id\": target_str,\n",
    "                        \"video_frame\": pvid.index,\n",
    "                        \"frames_per_second\": fps\n",
    "                    })\n",
    "\n",
    "                    # Mouse pair labels\n",
    "                    if mode == \"train\":\n",
    "                        labels = pd.DataFrame(0.0, index=pvid.index, columns=vid_agent_actions)\n",
    "                    \n",
    "                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n",
    "\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            labels.loc[annot_row[\"start_frame\"]:annot_row[\"stop_frame\"], annot_row.action] = 1.0\n",
    "                        yield \"pair\", mouse_pair, meta, labels\n",
    "                    else:\n",
    "                        yield \"pair\", mouse_pair, meta, vid_agent_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1703d6",
   "metadata": {
    "papermill": {
     "duration": 0.003728,
     "end_time": "2025-12-09T07:14:48.479777",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.476049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aeed244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.488316Z",
     "iopub.status.busy": "2025-12-09T07:14:48.487835Z",
     "iopub.status.idle": "2025-12-09T07:14:48.492163Z",
     "shell.execute_reply": "2025-12-09T07:14:48.491638Z"
    },
    "papermill": {
     "duration": 0.009707,
     "end_time": "2025-12-09T07:14:48.493211",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.483504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _fps_from_meta(meta_df, fallback_lookup, default_fps=30.0):\n",
    "    \"\"\"Get FPS with proper fallback chain\"\"\"\n",
    "    if \"frames_per_second\" in meta_df.columns:\n",
    "        fps_val = meta_df[\"frames_per_second\"].iloc[0]\n",
    "        if pd.notnull(fps_val) and fps_val > 0:\n",
    "            return float(fps_val)\n",
    "    \n",
    "    vid = meta_df[\"video_id\"].iloc[0]\n",
    "    if vid in fallback_lookup:\n",
    "        return float(fallback_lookup[vid])\n",
    "    \n",
    "    return default_fps\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    \"\"\"Scale window size by FPS\"\"\"\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c589eca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.503083Z",
     "iopub.status.busy": "2025-12-09T07:14:48.502775Z",
     "iopub.status.idle": "2025-12-09T07:14:48.518069Z",
     "shell.execute_reply": "2025-12-09T07:14:48.517460Z"
    },
    "papermill": {
     "duration": 0.021686,
     "end_time": "2025-12-09T07:14:48.519258",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.497572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_centers(df):\n",
    "    \"\"\"\n",
    "    Đảm bảo \"body_center\" tồn tại với mọi chuột hoặc bodyparts combination.\n",
    "    Xử lý cả cột 2-level (bodypart, coord) (từng chuột) và 3-level (mouse_id, bodypart, coord) (cặp chuột).\n",
    "    \n",
    "    Fallback logic:\n",
    "    1. Nếu nose và tail_base tồn tại → midpoint(nose, tail_base)\n",
    "    2. Else if head và tail_base tồn tại → midpoint(head, tail_base)\n",
    "    3. Else if chỉ tail_base tồn tại → use tail_base\n",
    "    4. Else → không tính được body_center\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "\n",
    "    # Cột 2-level (bodypart, coord)\n",
    "    if cols.nlevels == 2:\n",
    "        if (\"body_center\", \"x\") not in df.columns or (\"body_center\", \"y\") not in df.columns:\n",
    "            if (\"nose\", \"x\") in df.columns and (\"tail_base\", \"x\") in df.columns:\n",
    "                df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
    "                df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
    "            elif (\"head\", \"x\") in df.columns and (\"tail_base\", \"x\") in df.columns:\n",
    "                df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
    "                df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
    "            elif (\"tail_base\", \"x\") in df.columns:\n",
    "                df[(\"body_center\", \"x\")] = df[(\"tail_base\", \"x\")]\n",
    "                df[(\"body_center\", \"y\")] = df[(\"tail_base\", \"y\")]\n",
    "            else:\n",
    "                # no valid bodyparts → fill NaN\n",
    "                df[(\"body_center\", \"x\")] = np.nan\n",
    "                df[(\"body_center\", \"y\")] = np.nan\n",
    "\n",
    "    # Cột 3-level (mouse_id, bodypart, coord)\n",
    "    elif cols.nlevels == 3:\n",
    "        mice = sorted(list(set(c[0] for c in cols)))\n",
    "\n",
    "        for m in mice:\n",
    "            has_body_center = ((m, \"body_center\", \"x\") in cols) and ((m, \"body_center\", \"y\") in cols)\n",
    "            if not has_body_center:\n",
    "                if ((m, \"nose\", \"x\") in cols) and ((m, \"tail_base\", \"x\") in cols):\n",
    "                    df[(m, \"body_center\", \"x\")] = (df[(m, \"nose\", \"x\")] + df[(m, \"tail_base\", \"x\")]) / 2\n",
    "                    df[(m, \"body_center\", \"y\")] = (df[(m, \"nose\", \"y\")] + df[(m, \"tail_base\", \"y\")]) / 2\n",
    "                elif ((m, \"head\", \"x\") in cols) and ((m, \"tail_base\", \"x\") in cols):\n",
    "                    df[(m, \"body_center\", \"x\")] = (df[(m, \"head\", \"x\")] + df[(m, \"tail_base\", \"x\")]) / 2\n",
    "                    df[(m, \"body_center\", \"y\")] = (df[(m, \"head\", \"y\")] + df[(m, \"tail_base\", \"y\")]) / 2\n",
    "                elif ((m, \"tail_base\", \"x\") in cols):\n",
    "                    df[(m, \"body_center\", \"x\")] = df[(m, \"tail_base\", \"x\")]\n",
    "                    df[(m, \"body_center\", \"y\")] = df[(m, \"tail_base\", \"y\")]\n",
    "                else:\n",
    "                    df[(m, \"body_center\", \"x\")] = np.nan\n",
    "                    df[(m, \"body_center\", \"y\")] = np.nan\n",
    "    return df\n",
    "\n",
    "def calculate_speed_lag(df, part, fps, lag=30, mouse=None):\n",
    "    cols = df.columns\n",
    "    ws = _scale(lag, fps)\n",
    "    if mouse is not None:\n",
    "        x = df[(mouse, part, \"x\")]\n",
    "        y = df[(mouse, part, \"y\")]\n",
    "    else:\n",
    "        x = df[(part, \"x\")]\n",
    "        y = df[(part, \"y\")]\n",
    "\n",
    "    if x.isna().all() or y.isna().all():\n",
    "        # all missing → return zeros\n",
    "        return pd.Series(0, index=df.index)\n",
    "\n",
    "    dx = x.diff(ws)\n",
    "    dy = y.diff(ws)\n",
    "    speed = np.sqrt(dx**2 + dy**2)\n",
    "    return speed.fillna(0)\n",
    "\n",
    "# Tính các thống kê của 1 đại lượng theo nhiều cửa sổ thời gian\n",
    "def calculate_window_stats(df, metric, name, fps, scales=[30, 60, 90]):\n",
    "    \"\"\"\n",
    "    Thêm rolling statistics cho bất kỳ series nào.\n",
    "    \n",
    "    metric : pd.Series (ví dụ speed, distance, curvature...)\n",
    "    fps    : frames_per_second\n",
    "    scales : list window sizes quy đổi theo 30fps → mặc định [30, 60, 90] = short, medium và long term\n",
    "    \"\"\"\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    for scale in scales:\n",
    "        ws = _scale(scale, fps)\n",
    "        roll = metric.rolling(ws, min_periods=max(1, ws//4))\n",
    "\n",
    "        res[f\"{name}_mean{scale}\"] = roll.mean()\n",
    "        res[f\"{name}_std{scale}\"]  = roll.std()\n",
    "        res[f\"{name}_min{scale}\"]  = roll.min()\n",
    "        res[f\"{name}_max{scale}\"]  = roll.max()\n",
    "\n",
    "    return res\n",
    "\n",
    "# Tính onset - offset features: Onset = thay đổi từ {lag} frame trước -> frame hiện tại. Offset = thay đổi từ hiện tại -> {lag} frame tương lai\n",
    "def add_onset_offset(metric: pd.Series, name: str, fps, lag_list=[5, 10, 30, 60, 90]):\n",
    "    out = {}\n",
    "    for lag in lag_list:\n",
    "        ws = _scale(lag, fps)\n",
    "        out[f\"{name}_onset_lag{lag}\"]  = metric - metric.shift(ws).fillna(0)\n",
    "        out[f\"{name}_offset_lag{lag}\"] = metric.shift(-ws) - metric.fillna(0)\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6473d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.529562Z",
     "iopub.status.busy": "2025-12-09T07:14:48.529379Z",
     "iopub.status.idle": "2025-12-09T07:14:48.555713Z",
     "shell.execute_reply": "2025-12-09T07:14:48.555053Z"
    },
    "papermill": {
     "duration": 0.032913,
     "end_time": "2025-12-09T07:14:48.556851",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.523938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_single_features(single_mouse_df, body_parts_tracked, meta_fps):\n",
    "    single_mouse_df = calculate_centers(single_mouse_df)\n",
    "\n",
    "    # Lấy window sizes tương ứng body_parts_tracked\n",
    "    key = tuple(sorted(body_parts_tracked))\n",
    "    windows = CFG.window_map.get(key, CFG.default_window)\n",
    "\n",
    "    # Get actual bodypart columns\n",
    "    available_body_parts = single_mouse_df.columns.get_level_values(0).unique()\n",
    "    \n",
    "    # === Shape and Position Features ===\n",
    "    # Euclidean distances giữa các cặp bodyparts\n",
    "    X = pd.DataFrame({\n",
    "        f\"{p1}+{p2}\": np.sqrt(\n",
    "            (single_mouse_df[(p1, \"x\")] - single_mouse_df[(p2, \"x\")])**2 + \n",
    "            (single_mouse_df[(p1, \"y\")] - single_mouse_df[(p2, \"y\")])**2\n",
    "        )\n",
    "        for p1, p2 in itertools.combinations(body_parts_tracked, 2)\n",
    "        if p1 in available_body_parts and p2 in available_body_parts\n",
    "    })\n",
    "    \n",
    "    expected_cols = [f\"{p1}+{p2}\" for p1, p2 in itertools.combinations(body_parts_tracked, 2)]\n",
    "    X = X.reindex(columns=expected_cols, copy=False)\n",
    "    \n",
    "    # Elongation (chỉ khi required bodyparts tồn tại)\n",
    "    if \"nose\" in available_body_parts and \"tail_base\" in available_body_parts and \"ear_left\" in available_body_parts and \"ear_right\" in available_body_parts:\n",
    "        X[\"elong\"] = X[\"nose+tail_base\"] / (X[\"ear_left+ear_right\"] + 1e-6)\n",
    "    else:\n",
    "        X[\"elong\"] = 0.0\n",
    "    \n",
    "    # Body angle (chỉ khi nose, tail_base, body_center tồn tại)\n",
    "    if all(bp in available_body_parts for bp in [\"nose\", \"tail_base\", \"body_center\"]):\n",
    "        v1_x = single_mouse_df[(\"nose\",\"x\")] - single_mouse_df[(\"body_center\",\"x\")]\n",
    "        v1_y = single_mouse_df[(\"nose\",\"y\")] - single_mouse_df[(\"body_center\",\"y\")]\n",
    "        v2_x = single_mouse_df[(\"tail_base\",\"x\")] - single_mouse_df[(\"body_center\",\"x\")]\n",
    "        v2_y = single_mouse_df[(\"tail_base\",\"y\")] - single_mouse_df[(\"body_center\",\"y\")]\n",
    "        X[\"body_angle\"] = (v1_x*v2_x + v1_y*v2_y) / (np.sqrt(v1_x**2+v1_y**2) * np.sqrt(v2_x**2+v2_y**2) + 1e-6)\n",
    "    else:\n",
    "        X[\"body_angle\"] = 0.0\n",
    "\n",
    "    # Heading angle\n",
    "    if \"nose\" in available_body_parts and \"tail_base\" in available_body_parts:\n",
    "        X[\"heading_angle\"] = np.arctan2(\n",
    "            single_mouse_df[(\"nose\", \"y\")] - single_mouse_df[(\"tail_base\", \"y\")],\n",
    "            single_mouse_df[(\"nose\", \"x\")] - single_mouse_df[(\"tail_base\", \"x\")]\n",
    "        )\n",
    "        ws = _scale(30, meta_fps)\n",
    "        X[\"angular_velocity\"] = X[\"heading_angle\"].diff(ws)\n",
    "    else:\n",
    "        X[\"heading_angle\"] = 0.0\n",
    "        X[\"angular_velocity\"] = 0.0\n",
    "    \n",
    "    # === Movement Features: speed/accelerate/energy ===\n",
    "    if \"body_center\" in available_body_parts:\n",
    "        speed = np.sqrt(\n",
    "            single_mouse_df[(\"body_center\", \"x\")].diff()**2 +\n",
    "            single_mouse_df[(\"body_center\", \"y\")].diff()**2\n",
    "        ) * float(meta_fps)\n",
    "        X[\"speed\"] = speed\n",
    "        for window in windows:\n",
    "            ws = _scale(window, meta_fps)     # scale theo FPS\n",
    "            X[f\"speed_past{window}\"] = speed.rolling(ws).mean().fillna(0)\n",
    "\n",
    "        # Future context (15 frames)\n",
    "        ws_future = _scale(15, meta_fps)\n",
    "        X[\"speed_future15\"] = speed.shift(-ws_future).rolling(ws_future).mean().fillna(0)\n",
    "        \n",
    "        X[\"accelerate\"] = X[\"speed\"].diff().fillna(0)\n",
    "        X[\"jerk\"] = X[\"accelerate\"].diff().fillna(0)\n",
    "        for window in windows: \n",
    "            ws = _scale(window, meta_fps)\n",
    "            X[f\"energy_{window}\"] = (speed**2).rolling(ws).sum().fillna(0) # intensity of movement\n",
    "    else:\n",
    "        X[\"speed\"] = 0.0\n",
    "        for window in windows:\n",
    "            X[f\"speed_past{window}\"] = 0.0\n",
    "        X[\"speed_future15\"] = 0.0\n",
    "        X[\"accelerate\"] = 0.0\n",
    "        X[\"jerk\"] = 0.0\n",
    "        for window in windows:\n",
    "            X[f\"energy_{window}\"] = 0.0\n",
    "    \n",
    "    # Đối với available bodyparts cụ thể\n",
    "    for p in [\"body_center\"]:\n",
    "        if p in available_body_parts:\n",
    "            # Speed lag features\n",
    "            lag_speed = calculate_speed_lag(single_mouse_df, p, fps=meta_fps)\n",
    "            X[f\"speed_{p}_lag30\"] = lag_speed\n",
    "            X = pd.concat([X, add_onset_offset(lag_speed, f\"speed_{p}\", meta_fps)], axis=1)\n",
    "            \n",
    "            # Rolling stats\n",
    "            speed = np.sqrt(single_mouse_df[(p, \"x\")].diff()**2 + single_mouse_df[(p, \"y\")].diff()**2) * float(meta_fps)\n",
    "            res = calculate_window_stats(single_mouse_df, speed, f\"speed_{p}\", fps=meta_fps, scales=windows)\n",
    "            X = pd.concat([X, res], axis=1)\n",
    "\n",
    "            # Curvature\n",
    "            lag = _scale(30, meta_fps)\n",
    "            shifted_x = single_mouse_df[(p,\"x\")].shift(lag)\n",
    "            shifted_y = single_mouse_df[(p,\"y\")].shift(lag)\n",
    "            curv = np.sqrt((shifted_x - single_mouse_df[(p,\"x\")])**2 + \n",
    "                                                    (shifted_y - single_mouse_df[(p,\"y\")])**2)\n",
    "            X[f\"curvature_{p}_lag{lag}\"] = curv\n",
    "            X = pd.concat([X, add_onset_offset(curv, f\"curvature_{p}\", meta_fps)], axis=1)\n",
    "\n",
    "                \n",
    "        else:\n",
    "            # Dummy columns\n",
    "            X[f\"speed_{p}_lag30\"] = 0.0\n",
    "            for lag in [5, 10, 30, 60, 90]:\n",
    "                X[f\"speed{p}_onset_lag{lag}\"]  = 0.0\n",
    "                X[f\"speed_{p}_offset_lag{lag}\"] = 0.0\n",
    "            \n",
    "            \n",
    "            for scale in windows:\n",
    "                X[f\"speed_{p}_mean{scale}\"] = 0.0\n",
    "                X[f\"speed_{p}_std{scale}\"] = 0.0\n",
    "                X[f\"speed_{p}_min{scale}\"] = 0.0\n",
    "                X[f\"speed_{p}_max{scale}\"] = 0.0\n",
    "                \n",
    "            X[f\"curvature_{p}_lag{lag}\"] = 0.0\n",
    "\n",
    "            for lag in [5, 10, 30, 60, 90]:\n",
    "                X[f\"curvature_{p}_onset_lag{lag}\"]  = 0.0\n",
    "                X[f\"curvature_{p}_offset_lag{lag}\"] = 0.0\n",
    "    \n",
    "    return X.astype(np.float32, copy=False).fillna(0)\n",
    "\n",
    "\n",
    "def build_pair_features(mouse_pair_df, body_parts_tracked, meta_fps):\n",
    "    mouse_pair_df = calculate_centers(mouse_pair_df)\n",
    "\n",
    "    # Lấy window sizes tương ứng body_parts_tracked\n",
    "    key = tuple(sorted(body_parts_tracked))\n",
    "    windows = CFG.window_map.get(key, CFG.default_window)\n",
    "    \n",
    "    # Get bodyparts for both mice\n",
    "    avail_A = mouse_pair_df[\"A\"].columns.get_level_values(0).unique()\n",
    "    avail_B = mouse_pair_df[\"B\"].columns.get_level_values(0).unique()\n",
    "    \n",
    "    # Pairwise distances\n",
    "    X = pd.DataFrame({\n",
    "        f\"A_{p1}+B_{p2}\": np.sqrt(\n",
    "            (mouse_pair_df[(\"A\", p1, \"x\")] - mouse_pair_df[(\"B\", p2, \"x\")])**2 +\n",
    "            (mouse_pair_df[(\"A\", p1, \"y\")] - mouse_pair_df[(\"B\", p2, \"y\")])**2\n",
    "        )\n",
    "        for p1, p2 in itertools.product(body_parts_tracked, repeat=2)\n",
    "        if p1 in avail_A and p2 in avail_B\n",
    "    })\n",
    "    \n",
    "    expected_cols = [f\"A_{p1}+B_{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)]\n",
    "    X = X.reindex(columns=expected_cols, copy=False)\n",
    "    \n",
    "    # Relative orientation (chỉ khi nose and tail_base tồn tại ở cả 2 chuột)\n",
    "    if all(bp in avail_A for bp in [\"nose\",\"tail_base\"]) and all(bp in avail_B for bp in [\"nose\",\"tail_base\"]):\n",
    "        vec_A_x = mouse_pair_df[(\"A\", \"nose\", \"x\")] - mouse_pair_df[(\"A\", \"tail_base\", \"x\")]\n",
    "        vec_A_y = mouse_pair_df[(\"A\", \"nose\", \"y\")] - mouse_pair_df[(\"A\", \"tail_base\", \"y\")]\n",
    "        vec_B_x = mouse_pair_df[(\"B\", \"nose\", \"x\")] - mouse_pair_df[(\"B\", \"tail_base\", \"x\")]\n",
    "        vec_B_y = mouse_pair_df[(\"B\", \"nose\", \"y\")] - mouse_pair_df[(\"B\", \"tail_base\", \"y\")]\n",
    "        X[\"relative_orientation\"] = (vec_A_x*vec_B_x + vec_A_y*vec_B_y) / (\n",
    "            np.sqrt(vec_A_x**2 + vec_A_y**2) * np.sqrt(vec_B_x**2 + vec_B_y**2) + 1e-6\n",
    "        )\n",
    "    elif all(bp in avail_A for bp in [\"head\",\"tail_base\"]) and all(bp in avail_B for bp in [\"head\",\"tail_base\"]):\n",
    "        vec_A_x = mouse_pair_df[(\"A\", \"head\", \"x\")] - mouse_pair_df[(\"A\", \"tail_base\", \"x\")]\n",
    "        vec_A_y = mouse_pair_df[(\"A\", \"head\", \"y\")] - mouse_pair_df[(\"A\", \"tail_base\", \"y\")]\n",
    "        vec_B_x = mouse_pair_df[(\"B\", \"head\", \"x\")] - mouse_pair_df[(\"B\", \"tail_base\", \"x\")]\n",
    "        vec_B_y = mouse_pair_df[(\"B\", \"head\", \"y\")] - mouse_pair_df[(\"B\", \"tail_base\", \"y\")]\n",
    "        X[\"relative_orientation\"] = (vec_A_x*vec_B_x + vec_A_y*vec_B_y) / (\n",
    "            np.sqrt(vec_A_x**2 + vec_A_y**2) * np.sqrt(vec_B_x**2 + vec_B_y**2) + 1e-6\n",
    "        )\n",
    "    else:\n",
    "        X[\"relative_orientation\"] = 0.0\n",
    "    \n",
    "    # Center distance và approach\n",
    "    if \"body_center\" in avail_A and \"body_center\" in avail_B:\n",
    "        dist_center = np.sqrt(\n",
    "            (mouse_pair_df[(\"A\", \"body_center\", \"x\")] - mouse_pair_df[(\"B\", \"body_center\", \"x\")])**2 +\n",
    "            (mouse_pair_df[(\"A\", \"body_center\", \"y\")] - mouse_pair_df[(\"B\", \"body_center\", \"y\")])**2\n",
    "        )\n",
    "        X = pd.concat([X, add_onset_offset(dist_center, \"dist_center\", meta_fps)], axis=1)\n",
    "\n",
    "        ws = _scale(30, meta_fps)\n",
    "        approach = dist_center.diff(ws) * float(meta_fps)\n",
    "        X[\"approach_A\"] = approach\n",
    "        X[\"approach_B\"] = approach\n",
    "        X = pd.concat([X, add_onset_offset(approach, \"approach\", meta_fps)], axis=1)\n",
    "        \n",
    "        # Relative center distance stats\n",
    "        res = calculate_window_stats(mouse_pair_df, dist_center, \"center_distance\", fps=meta_fps, scales=windows)\n",
    "        X = pd.concat([X, res], axis=1)\n",
    "        \n",
    "        # Relative speed\n",
    "        speed_A = calculate_speed_lag(mouse_pair_df, \"body_center\", meta_fps, lag=30, mouse=\"A\")\n",
    "        speed_B = calculate_speed_lag(mouse_pair_df, \"body_center\", meta_fps, lag=30, mouse=\"B\")\n",
    "        X[\"speed_A_lag30\"] = speed_A\n",
    "        X[\"speed_B_lag30\"] = speed_B\n",
    "        rel = (speed_A - speed_B).abs()\n",
    "        X[\"relative_speed_A_B_lag30\"] = rel\n",
    "        X = pd.concat([X, add_onset_offset(rel, \"relative_speed\", meta_fps)], axis=1)\n",
    "\n",
    "        # Ngưỡng khoảng cách\n",
    "        thresholds = {\n",
    "            \"very_close\": 20,\n",
    "            \"close\": 40,\n",
    "            \"medium\": 60\n",
    "        }\n",
    "        X[\"very_close\"] = (dist_center < thresholds[\"very_close\"]).astype(float)\n",
    "        X[\"close\"] = ((dist_center >= thresholds[\"very_close\"]) & (dist_center < thresholds[\"close\"])).astype(float)\n",
    "        X[\"medium\"] = ((dist_center >= thresholds[\"close\"]) & (dist_center < thresholds[\"medium\"])).astype(float)\n",
    "        X[\"far\"] = (dist_center >= thresholds[\"medium\"]).astype(float)\n",
    "\n",
    "    else:\n",
    "        # Dummy columns\n",
    "        for lag in [5, 10, 30, 60, 90]:\n",
    "            X[f\"dist_center_onset_lag{lag}\"]  = 0.0\n",
    "            X[f\"dist_center_offset_lag{lag}\"] = 0.0\n",
    "\n",
    "        X[\"approach_A\"] = 0.0\n",
    "        X[\"approach_B\"] = 0.0\n",
    "        for lag in [5, 10, 30, 60, 90]:\n",
    "            X[f\"approach_onset_lag{lag}\"]  = 0.0\n",
    "            X[f\"approach_offset_lag{lag}\"] = 0.0\n",
    "        \n",
    "        for scale in windows:\n",
    "            X[f\"center_distance_mean{scale}\"] = 0.0\n",
    "            X[f\"center_distance_std{scale}\"] = 0.0\n",
    "            X[f\"center_distance_min{scale}\"] = 0.0\n",
    "            X[f\"center_distance_max{scale}\"] = 0.0\n",
    "        \n",
    "        X[\"speed_A_lag30\"] = 0.0\n",
    "        X[\"speed_B_lag30\"] = 0.0\n",
    "        X[\"relative_speed_A_B_lag30\"] = 0.0\n",
    "        for lag in [5, 10, 30, 60, 90]:\n",
    "            X[f\"relative_speed_onset_lag{lag}\"]  = 0.0\n",
    "            X[f\"relative_speed_offset_lag{lag}\"] = 0.0\n",
    "\n",
    "        X[\"very_close\"] = 0.0\n",
    "        X[\"close\"] = 0.0\n",
    "        X[\"medium\"] = 0.0\n",
    "        X[\"far\"] = 0.0   \n",
    "    \n",
    "    return X.astype(np.float32, copy=False).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c53c8",
   "metadata": {
    "papermill": {
     "duration": 0.00378,
     "end_time": "2025-12-09T07:14:48.564601",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.560821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2e2fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.573345Z",
     "iopub.status.busy": "2025-12-09T07:14:48.573129Z",
     "iopub.status.idle": "2025-12-09T07:14:48.588951Z",
     "shell.execute_reply": "2025-12-09T07:14:48.588283Z"
    },
    "papermill": {
     "duration": 0.021582,
     "end_time": "2025-12-09T07:14:48.589993",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.568411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StratifiedSubsetClassifier(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Wrapper class để subsamples data trước khi train\n",
    "    Update: Thử phương pháp adaptive: xác định các target ratio (giữa positive frames / total frames) dựa trên imbalance của action đó\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_samples=None, min_target_ratio=0.15, max_target_ratio=0.30, min_samples_per_class=1000):\n",
    "        self.estimator = estimator\n",
    "        self.n_samples = n_samples\n",
    "        self.min_target_ratio = min_target_ratio\n",
    "        self.max_target_ratio = max_target_ratio\n",
    "        self.min_samples_per_class = min_samples_per_class\n",
    "\n",
    "    def _to_numpy(self, X):\n",
    "        try:\n",
    "            return X.to_numpy(np.float32, copy=False)\n",
    "        except AttributeError:\n",
    "            return np.asarray(X, dtype=np.float32)\n",
    "\n",
    "    def _calculate_target_ratio(self, current_ratio):\n",
    "        \"\"\"\n",
    "        Calculate optimal target ratio based on current imbalance severity.\n",
    "        \n",
    "        Strategy logic:\n",
    "        - Extreme (<1%): Boost strongly (3x) → ~15-20%\n",
    "        - Very severe (1-5%): Significant boost (2.0x) → ~15-20%\n",
    "        - Severe (5-10%): Moderate boost (1.8x) → ~15-20%\n",
    "        - Moderate (10-15%): Gentle boost (1.4x) → ~15-22%\n",
    "        - Acceptable (>15%): Minimal or no boost (1.15x) → ~17-25%\n",
    "        \n",
    "        Rationale for not going higher:\n",
    "        - Action detection: Negative frames (no action) are still informative\n",
    "        - Too much undersampling loses context about \"normal\" behavior\n",
    "        \"\"\"\n",
    "        if current_ratio < 0.01:\n",
    "            target = max(self.min_target_ratio, current_ratio * 3.0) # very strong boost\n",
    "        elif current_ratio < 0.03:\n",
    "            target = current_ratio * 2.0\n",
    "            target = max(target, self.min_target_ratio) # strong boost\n",
    "        elif current_ratio < 0.05:\n",
    "            target = current_ratio * 1.8\n",
    "            target = max(target, self.min_target_ratio) # significant boost\n",
    "        elif current_ratio < 0.10:\n",
    "            target = current_ratio * 1.4 # moderate boost\n",
    "        elif current_ratio < 0.15:\n",
    "            target = current_ratio * 1.15 # gentle boost\n",
    "        else:\n",
    "            target = current_ratio # no adjustment\n",
    "\n",
    "        # Áp dụng cap để tránh over-correction\n",
    "        target = min(target, self.max_target_ratio)\n",
    "\n",
    "        return target\n",
    "\n",
    "    def _smart_sample(self, X_np, y, target_samples):\n",
    "        \"\"\"\n",
    "        Custom balanced sampling to achieve target positive ratio\n",
    "        \"\"\"\n",
    "        pos_indices = np.where(y == 1)[0]\n",
    "        neg_indices = np.where(y == 0)[0]\n",
    "        \n",
    "        pos_count = len(pos_indices)\n",
    "        neg_count = len(neg_indices)\n",
    "        total_count = pos_count + neg_count\n",
    "\n",
    "        if total_count == 0:\n",
    "            raise ValueError(\"No valid samples found\")\n",
    "\n",
    "        current_ratio = pos_count / total_count\n",
    "\n",
    "        # Determine target ratio based on strategy\n",
    "        target_ratio = self._calculate_target_ratio(current_ratio)\n",
    "        \n",
    "        # Calculate desired sample counts\n",
    "        target_pos = int(target_samples * target_ratio)\n",
    "        target_neg = target_samples - target_pos\n",
    "        \n",
    "        # Áp dụng minimum samples constraint\n",
    "        target_pos = max(target_pos, self.min_samples_per_class)\n",
    "        target_neg = max(target_neg, self.min_samples_per_class)\n",
    "        \n",
    "        # Key strategy: Giữ tất cả positives nếu quá ít, giảm negatives\n",
    "        if pos_count <= target_pos:\n",
    "            # Nếu positives < target thì giữ tất\n",
    "            sampled_pos = pos_indices\n",
    "            actual_pos = pos_count\n",
    "            \n",
    "            # Điều chỉnh số negatives để đạt target ratio\n",
    "            # target_ratio = actual_pos / (actual_pos + actual_neg)\n",
    "            actual_neg = int(actual_pos * (1 - target_ratio) / target_ratio)\n",
    "            actual_neg = min(actual_neg, neg_count)\n",
    "            actual_neg = max(actual_neg, self.min_samples_per_class)\n",
    "            \n",
    "        else:\n",
    "            # Đủ số positives thì sample được cả pos lẫn neg\n",
    "            actual_pos = target_pos\n",
    "            actual_neg = target_neg\n",
    "            \n",
    "            # Đảm bảo không vượt quá số samples hiện tại\n",
    "            actual_pos = min(actual_pos, pos_count)\n",
    "            actual_neg = min(actual_neg, neg_count)\n",
    "            \n",
    "            sampled_pos = np.random.choice(pos_indices, actual_pos, replace=False)\n",
    "        \n",
    "        # Sample negatives\n",
    "        if actual_neg < neg_count:\n",
    "            sampled_neg = np.random.choice(neg_indices, actual_neg, replace=False)\n",
    "        else:\n",
    "            sampled_neg = neg_indices\n",
    "            actual_neg = len(sampled_neg)\n",
    "        \n",
    "        # Final achieved ratio\n",
    "        total_sampled = actual_pos + actual_neg\n",
    "        achieved_ratio = actual_pos / total_sampled if total_sampled > 0 else 0\n",
    "        \n",
    "        # Combine và shuffle\n",
    "        selected_indices = np.concatenate([sampled_pos, sampled_neg])\n",
    "        np.random.shuffle(selected_indices)\n",
    "        \n",
    "        return selected_indices\n",
    "\n",
    "    def _fallback_fit(self, X_np, y, target_samples):\n",
    "        \"\"\"\n",
    "        Fallback fitting strategy\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, train_size=target_samples, random_state=42)\n",
    "            idx, _ = next(sss.split(np.zeros_like(y), y))\n",
    "            self.estimator.fit(X_np[idx], y[idx])\n",
    "        except Exception:\n",
    "            step = max(len(X_np) // target_samples, 1)\n",
    "            self.estimator.fit(X_np[::step], y[::step])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_np = self._to_numpy(X)\n",
    "        y = np.asarray(y).ravel()\n",
    "\n",
    "        # Handle edge case: labels có thể là {0, 2} thay vì {0, 1}\n",
    "        uniq = np.unique(y[~pd.isna(y)])\n",
    "        if set(uniq.tolist()) == {0, 2}:\n",
    "            y = (y > 0).astype(np.int8)\n",
    "\n",
    "        should_sample = (self.n_samples is not None and len(X_np) > int(self.n_samples))\n",
    "\n",
    "        if not should_sample:\n",
    "            self.estimator.fit(X_np, y)\n",
    "        else:\n",
    "            target_samples = int(self.n_samples)\n",
    "            try:\n",
    "                indices = self._smart_sample(X_np, y, target_samples)\n",
    "                self.estimator.fit(X_np[indices], y[indices])\n",
    "            except Exception as e:\n",
    "                print(f\"  Sampling failed: {str(e)[:100]}\")\n",
    "                self._fallback_fit(X_np, y, target_samples)\n",
    "\n",
    "        try:\n",
    "            self.classes_ = np.asarray(self.estimator.classes_)\n",
    "        except Exception:\n",
    "            self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_np = self._to_numpy(X)\n",
    "        try:\n",
    "            P = self.estimator.predict_proba(X_np)\n",
    "        except Exception:\n",
    "            # Handle single class case\n",
    "            if len(self.classes_) == 1:\n",
    "                n = len(X_np)\n",
    "                c = int(self.classes_[0])\n",
    "                if c == 1:\n",
    "                    return np.column_stack([np.zeros(n, dtype=np.float32), np.ones(n, dtype=np.float32)])\n",
    "                else:\n",
    "                    return np.column_stack([np.ones(n, dtype=np.float32), np.zeros(n, dtype=np.float32)])\n",
    "            return np.full((len(X_np), 2), 0.5, dtype=np.float32)\n",
    "\n",
    "        P = np.asarray(P)\n",
    "        if P.ndim == 1:\n",
    "            P1 = P.astype(np.float32)\n",
    "            return np.column_stack([1.0 - P1, P1])\n",
    "        if P.shape[1] == 1 and len(self.classes_) == 2:\n",
    "            P1 = P[:, 0].astype(np.float32)\n",
    "            return np.column_stack([1.0 - P1, P1])\n",
    "        return P\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_np = self._to_numpy(X)\n",
    "        try:\n",
    "            return self.estimator.predict(X_np)\n",
    "        except Exception:\n",
    "            return np.argmax(self.predict_proba(X_np), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3339753e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.598615Z",
     "iopub.status.busy": "2025-12-09T07:14:48.598417Z",
     "iopub.status.idle": "2025-12-09T07:14:48.603581Z",
     "shell.execute_reply": "2025-12-09T07:14:48.602941Z"
    },
    "papermill": {
     "duration": 0.010703,
     "end_time": "2025-12-09T07:14:48.604613",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.593910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_threshold_map(thresholds, mode: str):\n",
    "    \"\"\"\n",
    "    Tạo 1 defaultdict có mode(single/pair)-aware và action-specific thresholds\n",
    "    \"\"\"\n",
    "    if isinstance(thresholds, dict):\n",
    "        # Kiểm tra mode-aware structure tồn tại\n",
    "        if (\"single\" in thresholds) or (\"pair\" in thresholds) or \\\n",
    "           (\"single_default\" in thresholds) or (\"pair_default\" in thresholds):\n",
    "            # Mode-aware thresholds\n",
    "            base_default = float(thresholds.get(\"default\", 0.27))\n",
    "            mode_default = float(thresholds.get(f\"{mode}_default\", base_default))\n",
    "            mode_overrides = thresholds.get(mode, {}) or {}\n",
    "            \n",
    "            out = defaultdict(lambda: mode_default)\n",
    "            out.update({str(k): float(v) for k, v in mode_overrides.items()})\n",
    "            return out\n",
    "        \n",
    "        # Plain per-action dict\n",
    "        out = defaultdict(lambda: float(thresholds.get(\"default\", 0.27)))\n",
    "        out.update({str(k): float(v) for k, v in thresholds.items() if k != \"default\"})\n",
    "        return out\n",
    "    \n",
    "    # Fallback: constant threshold\n",
    "    return defaultdict(lambda: 0.27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5d7d3",
   "metadata": {
    "papermill": {
     "duration": 0.003928,
     "end_time": "2025-12-09T07:14:48.612656",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.608728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91510620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.621664Z",
     "iopub.status.busy": "2025-12-09T07:14:48.621445Z",
     "iopub.status.idle": "2025-12-09T07:14:48.640422Z",
     "shell.execute_reply": "2025-12-09T07:14:48.639781Z"
    },
    "papermill": {
     "duration": 0.024825,
     "end_time": "2025-12-09T07:14:48.641407",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.616582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_actions_ensemble(X, models_dict, actions):\n",
    "    \"\"\"\n",
    "    Dự đoán nhiều action với ensemble models.\n",
    "    \n",
    "    Args:\n",
    "        X: feature DataFrame\n",
    "        models: dict of trained classifiers {action: model}\n",
    "        actions: danh sách actions để predict\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame với xác suất mỗi actions\n",
    "    \"\"\"\n",
    "    \n",
    "    proba_df = pd.DataFrame(index=X.index)\n",
    "    X_np = X.to_numpy(np.float32, copy=False)\n",
    "    \n",
    "    for action in actions:\n",
    "        if action not in models_dict:\n",
    "            # Action not trained in this section\n",
    "            proba_df[action] = 0.0\n",
    "            continue\n",
    "        \n",
    "        model_list = models_dict[action]\n",
    "        \n",
    "        try:\n",
    "            # Get predictions from all models in ensemble\n",
    "            probs_list = []\n",
    "            for model in model_list:\n",
    "                try:\n",
    "                    prob = model.predict_proba(X_np)[:, 1]\n",
    "                    probs_list.append(prob)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            if len(probs_list) > 0:\n",
    "                # Average ensemble predictions\n",
    "                proba_df[action] = np.mean(probs_list, axis=0)\n",
    "            else:\n",
    "                proba_df[action] = 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            proba_df[action] = 0.0\n",
    "    \n",
    "    return proba_df\n",
    "\n",
    "\n",
    "def predict_multiclass_adaptive(pred, meta, action_thresholds):\n",
    "    \"\"\"\n",
    "    Đổi frame probabilities thành chuỗi frame có action đó\n",
    "    + Adaptive thresholding cho từng action + bilateral smoothing\n",
    "    Args:\n",
    "        pred: DataFrame (num_frames, num_actions) with probabilities\n",
    "        meta: DataFrame with video_frame, agent_id, target_id\n",
    "        thresholds: dict of thresholds per action\n",
    "    Returns:\n",
    "        DataFrame with columns: video_id, agent_id, target_id, action, start_frame, stop_frame\n",
    "    \"\"\"\n",
    "    # === Áp dụng temporal smoothing -> đổi thành bilateral smoothing giúp giữ edges tốt hơn rolling mean ===\n",
    "    pred_smoothed = pred.copy()\n",
    "    for col in pred.columns:\n",
    "        # Gaussian filter với sigma=2 (tương đương ~5 frame window)\n",
    "        pred_smoothed[col] = gaussian_filter1d(pred[col].values, sigma=2, mode='nearest')\n",
    "    \n",
    "    # Thêm median filter để bỏ outliers\n",
    "    pred_smoothed = pred_smoothed.rolling(window=3, min_periods=1, center=True).median()\n",
    "\n",
    "    # Determine mode (single/pair)\n",
    "    mode = \"pair\"\n",
    "    try:\n",
    "        if \"target_id\" in meta.columns and meta[\"target_id\"].eq(\"self\").all():\n",
    "            mode = \"single\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # === Áp dụng Adaptive thresholding ===\n",
    "    # Lấy threshold map cho mode tương ứng\n",
    "    th_map = select_threshold_map(action_thresholds, mode)\n",
    "\n",
    "    # Áp dụng per-action thresholds\n",
    "    ama = np.argmax(pred_smoothed.values, axis=1)\n",
    "    max_probs = pred_smoothed.max(axis=1).values\n",
    "\n",
    "    # Áp dụng thresholds\n",
    "    threshold_mask = np.zeros(len(pred_smoothed), dtype=bool)\n",
    "    for i, action in enumerate(pred_smoothed.columns):\n",
    "        action_mask = (ama == i)\n",
    "        threshold = th_map[action]\n",
    "        threshold_mask |= (action_mask & (max_probs >= threshold))\n",
    "    ama = np.where(threshold_mask, ama, -1)\n",
    "    ama = pd.Series(ama, index=meta.video_frame.values)\n",
    "    \n",
    "    # === Detect changes ===\n",
    "    changes_mask = (ama != ama.shift(1)).values\n",
    "    ama_changes = ama[changes_mask]\n",
    "    meta_changes = meta[changes_mask]\n",
    "    mask = ama_changes.values >= 0\n",
    "    mask[-1] = False\n",
    "    \n",
    "    submission_part = pd.DataFrame({\n",
    "        \"video_id\": meta_changes[\"video_id\"].values[mask],\n",
    "        \"agent_id\": meta_changes[\"agent_id\"].values[mask],\n",
    "        \"target_id\": meta_changes[\"target_id\"].values[mask],\n",
    "        \"action\": pred.columns[ama_changes.values[mask]],\n",
    "        \"start_frame\": ama_changes.index[mask],\n",
    "        \"stop_frame\": ama_changes.index[1:][mask[:-1]]\n",
    "    })\n",
    "    \n",
    "    # === Fix stop_frame với mỗi bộ video/agent/target ===\n",
    "    stop_video_id = meta_changes[\"video_id\"].values[1:][mask[:-1]]\n",
    "    stop_agent_id = meta_changes[\"agent_id\"].values[1:][mask[:-1]]\n",
    "    stop_target_id = meta_changes[\"target_id\"].values[1:][mask[:-1]]\n",
    "    \n",
    "    for i in range(len(submission_part)):\n",
    "        video_id = submission_part.video_id.iloc[i]\n",
    "        agent_id = submission_part.agent_id.iloc[i]\n",
    "        target_id = submission_part.target_id.iloc[i]\n",
    "        \n",
    "        if i < len(stop_video_id):\n",
    "            if stop_video_id[i] != video_id or stop_agent_id[i] != agent_id or stop_target_id[i] != target_id:\n",
    "                new_stop_frame = meta.query(\"(video_id == @video_id)\").video_frame.max() + 1\n",
    "                submission_part.iat[i, submission_part.columns.get_loc(\"stop_frame\")] = new_stop_frame\n",
    "        else:\n",
    "            meta.query(\"(video_id == @video_id)\").video_frame.max() + 1\n",
    "            submission_part.iat[i, submission_part.columns.get_loc(\"stop_frame\")] = new_stop_frame\n",
    "\n",
    "    if len(submission_part) == 0:\n",
    "        return submission_part\n",
    "\n",
    "    # === Áp dụng min duration filtering ===\n",
    "    min_dur_map = CFG.min_duration.get(mode, {})\n",
    "    \n",
    "    duration = submission_part.stop_frame - submission_part.start_frame\n",
    "    # submission_part = submission_part[duration >= 3].reset_index(drop=True)\n",
    "    valid_mask = np.ones(len(submission_part), dtype=bool)\n",
    "    for idx, row in submission_part.iterrows():\n",
    "        action = row[\"action\"]\n",
    "        min_dur = min_dur_map.get(action, CFG.default_min_dur)\n",
    "        if duration.loc[idx] < min_dur:\n",
    "            valid_mask[idx] = False\n",
    "    \n",
    "    submission_part = submission_part[valid_mask].reset_index(drop=True)\n",
    "\n",
    "    if len(submission_part) == 0:\n",
    "        return submission_part\n",
    "\n",
    "    # === Merge events của cùng 1 action mà cách nhau khoảng nhỏ (5 frames) ===\n",
    "    MERGE_GAP = 5\n",
    "    merged_list = []\n",
    "    for (video_id, agent_id, target_id, action), group in submission_part.groupby(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\"]\n",
    "    ):\n",
    "        group = group.sort_values(\"start_frame\").reset_index(drop=True)\n",
    "        \n",
    "        if len(group) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Bắt đầu = first event\n",
    "        current_start = group.iloc[0][\"start_frame\"]\n",
    "        current_stop = group.iloc[0][\"stop_frame\"]\n",
    "        \n",
    "        for i in range(1, len(group)):\n",
    "            gap = group.iloc[i][\"start_frame\"] - current_stop\n",
    "            \n",
    "            if gap <= MERGE_GAP:\n",
    "                # Merge: extend current event\n",
    "                current_stop = group.iloc[i][\"stop_frame\"]\n",
    "            else:\n",
    "                # Save current event and start new one\n",
    "                merged_list.append({\n",
    "                    \"video_id\": video_id,\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"action\": action,\n",
    "                    \"start_frame\": current_start,\n",
    "                    \"stop_frame\": current_stop\n",
    "                })\n",
    "                current_start = group.iloc[i][\"start_frame\"]\n",
    "                current_stop = group.iloc[i][\"stop_frame\"]\n",
    "\n",
    "        # Thêm last event\n",
    "        merged_list.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"agent_id\": agent_id,\n",
    "            \"target_id\": target_id,\n",
    "            \"action\": action,\n",
    "            \"start_frame\": current_start,\n",
    "            \"stop_frame\": current_stop\n",
    "        })\n",
    "    \n",
    "    if len(merged_list) > 0:\n",
    "        submission_part = pd.DataFrame(merged_list)\n",
    "    else:\n",
    "        submission_part = pd.DataFrame(columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "\n",
    "    # === Bỏ overlapping events: Nếu có overlaps, giữ event có confidence cao hơn ===\n",
    "    cleaned_list = []\n",
    "    for (video_id, agent_id, target_id), group in submission_part.groupby(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\"]\n",
    "    ):\n",
    "        group = group.sort_values(\"start_frame\").reset_index(drop=True)\n",
    "        \n",
    "        keep_mask = np.ones(len(group), dtype=bool)\n",
    "        \n",
    "        for i in range(len(group) - 1):\n",
    "            if not keep_mask[i]:\n",
    "                continue\n",
    "                \n",
    "            for j in range(i + 1, len(group)):\n",
    "                if not keep_mask[j]:\n",
    "                    continue\n",
    "                \n",
    "                # Kiểm tra overlap\n",
    "                start_i, stop_i = group.iloc[i][\"start_frame\"], group.iloc[i][\"stop_frame\"]\n",
    "                start_j, stop_j = group.iloc[j][\"start_frame\"], group.iloc[j][\"stop_frame\"]\n",
    "                \n",
    "                if start_j < stop_i:  # Overlap detected\n",
    "                    # Giữ events dài hơn (confidence cao hơn)\n",
    "                    dur_i = stop_i - start_i\n",
    "                    dur_j = stop_j - start_j\n",
    "                    \n",
    "                    if dur_i >= dur_j:\n",
    "                        keep_mask[j] = False\n",
    "                    else:\n",
    "                        keep_mask[i] = False\n",
    "                        break\n",
    "        \n",
    "        cleaned_list.append(group[keep_mask])\n",
    "    \n",
    "    if len(cleaned_list) > 0:\n",
    "        submission_part = pd.concat(cleaned_list, ignore_index=True)\n",
    "    else:\n",
    "        submission_part = pd.DataFrame(columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "\n",
    "    # === Kiểm tra cuối ===\n",
    "    if len(submission_part) > 0:\n",
    "        # Đảm bảo stop > start\n",
    "        valid_duration = submission_part.stop_frame > submission_part.start_frame\n",
    "        submission_part = submission_part[valid_duration].reset_index(drop=True)\n",
    "        \n",
    "        # Sắp xếp theo video, agent, start_frame\n",
    "        submission_part = submission_part.sort_values(\n",
    "            [\"video_id\", \"agent_id\", \"target_id\", \"start_frame\"]\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        assert (submission_part.stop_frame > submission_part.start_frame).all(), \"Invalid: stop <= start\"\n",
    "    \n",
    "    return submission_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b60d76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.650328Z",
     "iopub.status.busy": "2025-12-09T07:14:48.650116Z",
     "iopub.status.idle": "2025-12-09T07:14:48.659156Z",
     "shell.execute_reply": "2025-12-09T07:14:48.658474Z"
    },
    "papermill": {
     "duration": 0.014844,
     "end_time": "2025-12-09T07:14:48.660217",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.645373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_submission(submission, dataset, mode, traintest_directory=None):\n",
    "    \"\"\"\n",
    "    Làm sạch submission:\n",
    "    1. Bỏ các chuỗi start_frame >= stop_frame\n",
    "    2. Bỏ các chuỗi bị lặp (cùng agent-target)\n",
    "    3. Điền video trống với dummy predictions\n",
    "    \"\"\"\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{mode}_tracking\"\n",
    "        # traintest_directory = f\"D:/UET/ML/mouse_behavior/data/{mode}_tracking\"\n",
    "    \n",
    "    # Bỏ invalid frames\n",
    "    submission = submission[submission.start_frame < submission.stop_frame].copy()\n",
    "\n",
    "    # Bỏ rows có NaN\n",
    "    submission = submission.dropna(subset=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    # Bỏ chuỗi bị lặp\n",
    "    group_list = []\n",
    "    for _, group in submission.groupby([\"video_id\", \"agent_id\", \"target_id\"]):\n",
    "        group = group.sort_values(\"start_frame\")\n",
    "        mask = np.ones(len(group), dtype=bool)\n",
    "        last_stop_frame = 0\n",
    "        for i, (_, row) in enumerate(group.iterrows()):\n",
    "            if row[\"start_frame\"] < last_stop_frame:\n",
    "                mask[i] = False\n",
    "            else:\n",
    "                last_stop_frame = row[\"stop_frame\"]\n",
    "        group_list.append(group[mask])\n",
    "    \n",
    "    submission = pd.concat(group_list)\n",
    "\n",
    "    if len(group_list) > 0:\n",
    "        submission = pd.concat(group_list, ignore_index=True)\n",
    "    else:\n",
    "        submission = pd.DataFrame(columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    # Điền video trống\n",
    "    s_list = []\n",
    "    for idx, row in dataset.iterrows():\n",
    "        lab_id = row[\"lab_id\"]\n",
    "        if lab_id.startswith('MABe22'):\n",
    "            continue\n",
    "        \n",
    "        video_id = row[\"video_id\"]\n",
    "        if (submission.video_id == video_id).any():\n",
    "            continue\n",
    "\n",
    "        if type(row.behaviors_labeled) != str:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Video {video_id} has no predictions, filling...\")\n",
    "        \n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        try:\n",
    "            vid = pd.read_parquet(path)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        vid_behaviors = json.loads(row[\"behaviors_labeled\"])\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=[\"agent\", \"target\", \"action\"])\n",
    "        \n",
    "        start_frame = vid.video_frame.min()\n",
    "        stop_frame = vid.video_frame.max() + 1\n",
    "        \n",
    "        for (agent, target), actions in vid_behaviors.groupby([\"agent\", \"target\"]):\n",
    "            batch_length = int(np.ceil((stop_frame - start_frame) / len(actions)))\n",
    "            for i, (_, action_row) in enumerate(actions.iterrows()):\n",
    "                batch_start = start_frame + i * batch_length\n",
    "                batch_stop = min(batch_start + batch_length, stop_frame)\n",
    "                s_list.append((video_id, agent, target, action_row[\"action\"], batch_start, batch_stop))\n",
    "    \n",
    "    if len(s_list) > 0:\n",
    "        submission = pd.concat([\n",
    "            submission,\n",
    "            pd.DataFrame(s_list, columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "        ])\n",
    "    \n",
    "    submission = submission.reset_index(drop=True)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560b0f1",
   "metadata": {
    "papermill": {
     "duration": 0.003871,
     "end_time": "2025-12-09T07:14:48.668549",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.664678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pipeline train-and-submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df31e0fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.677481Z",
     "iopub.status.busy": "2025-12-09T07:14:48.677255Z",
     "iopub.status.idle": "2025-12-09T07:14:48.689054Z",
     "shell.execute_reply": "2025-12-09T07:14:48.688349Z"
    },
    "papermill": {
     "duration": 0.017734,
     "end_time": "2025-12-09T07:14:48.690189",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.672455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_models_and_submit_ensemble(section, body_parts_tracked_str, mode_train, X_all, y_all, meta_all, n_samples):\n",
    "    \"\"\"\n",
    "    Theo từng section/body_part_tracked, train binary classifiers cho từng action (trên bộ train) và predict (trên bộ test).\n",
    "    \"\"\"\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    # Check GPU availability\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "\n",
    "    X_all_np = X_all.to_numpy(np.float32, copy=False)\n",
    "    del X_all\n",
    "    gc.collect()\n",
    "\n",
    "    # Model 1: LightGBM\n",
    "    models.append(\n",
    "        StratifiedSubsetClassifier(\n",
    "            lgb.LGBMClassifier(\n",
    "                n_estimators=400,\n",
    "                learning_rate=0.07,\n",
    "                num_leaves=31,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                device_type=\"gpu\" if gpu_available else \"cpu\",\n",
    "                verbose=-1,\n",
    "                random_state=42\n",
    "            ), n_samples = int(n_samples / 1.3), min_target_ratio=0.15, max_target_ratio=0.30, min_samples_per_class=1000\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Model 2: LightGBM\n",
    "    models.append(make_pipeline(\n",
    "        StratifiedSubsetClassifier(\n",
    "            lgb.LGBMClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.1,\n",
    "                num_leaves=63,\n",
    "                max_depth=8,\n",
    "                device_type=\"gpu\" if gpu_available else \"cpu\",\n",
    "                verbose=-1,\n",
    "                random_state=42\n",
    "            ), n_samples = int(n_samples / 1.5), min_target_ratio=0.15, max_target_ratio=0.30, min_samples_per_class=1000\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    # Model 3: XGBoost\n",
    "    models.append(make_pipeline(\n",
    "        StratifiedSubsetClassifier(\n",
    "            xgb.XGBClassifier(\n",
    "                n_estimators=400,\n",
    "                learning_rate=0.08,\n",
    "                max_depth=6,\n",
    "                tree_method=\"gpu_hist\" if gpu_available else \"hist\",\n",
    "                device=\"cuda\" if gpu_available else \"cpu\",\n",
    "                random_state=42\n",
    "            ), n_samples = int(n_samples / 1.3), min_target_ratio=0.15, max_target_ratio=0.30, min_samples_per_class=1000\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    model_list = []\n",
    "    for action in y_all.columns:\n",
    "        y_raw = y_all[action].to_numpy()\n",
    "        mask = ~pd.isna(y_raw)\n",
    "        y_action = y_raw[mask].astype(int)\n",
    "        \n",
    "        if not (y_action == 0).all() and np.sum(y_action) >= 5:\n",
    "            trained = []\n",
    "            idx = np.flatnonzero(mask)\n",
    "            for m in models:\n",
    "                m_clone = clone(m)\n",
    "                m_clone.fit(X_all_np[idx], y_action)\n",
    "                trained.append(m_clone)\n",
    "            model_list.append((action, trained))\n",
    "\n",
    "    del X_all_np\n",
    "    gc.collect()\n",
    "\n",
    "    # Test tune threshold\n",
    "    if CFG.tune_thresholds_mode:\n",
    "        import joblib\n",
    "        import os\n",
    "        os.makedirs(f\"{CFG.model_save_dir}/models\", exist_ok=True)\n",
    "        \n",
    "        models_dict = {action: trained for action, trained in model_list}\n",
    "        \n",
    "        filename = f\"{CFG.model_save_dir}/models/{mode_train}_{section}.pkl\"\n",
    "        joblib.dump(models_dict, filename)\n",
    "        print(f\"  Saved models to {filename}\")\n",
    "        return  # Exit early, don't predict\n",
    "\n",
    "\n",
    "    # Build test data của body_part_tracked hiện tại\n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "    if len(body_parts_tracked) > 5:\n",
    "        body_parts_tracked = [b for b in body_parts_tracked if b not in CFG.drop_body_parts]\n",
    "    \n",
    "    test_subset = test_csv[test_csv.body_parts_tracked == body_parts_tracked_str]\n",
    "\n",
    "    # Predict single\n",
    "    generator = generate_mouse_data(\n",
    "        test_subset, \n",
    "        mode=\"test\", \n",
    "        generate_single=(mode_train == \"single\"), \n",
    "        generate_pair=(mode_train == \"pair\")\n",
    "    )\n",
    "\n",
    "    # Tạo fps_lookup cho test set\n",
    "    fps_lookup = (\n",
    "        test_subset[[\"video_id\", \"frames_per_second\"]]\n",
    "        .drop_duplicates(\"video_id\")\n",
    "        .set_index(\"video_id\")[\"frames_per_second\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    for mode_test, data_test, meta_test, actions_test in generator:\n",
    "        assert mode_test == mode_train\n",
    "        fps_i = _fps_from_meta(meta_test, fps_lookup, default_fps=30.0)\n",
    "\n",
    "        # Feature engineering\n",
    "        if mode_test == \"single\":\n",
    "            X_test = build_single_features(data_test, body_parts_tracked, fps_i).astype(np.float32)\n",
    "        else:\n",
    "            X_test = build_pair_features(data_test, body_parts_tracked, fps_i).astype(np.float32)\n",
    "\n",
    "        X_test_np = X_test.to_numpy(np.float32, copy=False)\n",
    "        del data_test\n",
    "        gc.collect()\n",
    "\n",
    "        # Predict\n",
    "        # preds = predict_actions_ensemble(X_test, model_list, actions_test)\n",
    "\n",
    "        pred = pd.DataFrame(index=meta_test.video_frame)\n",
    "        for action, trained in model_list:\n",
    "            if action in actions_test:\n",
    "                probs = [m.predict_proba(X_test_np)[:, 1] for m in trained]\n",
    "                pred[action] = np.average(probs, axis=0)\n",
    "\n",
    "        del X_test_np\n",
    "        gc.collect()\n",
    "\n",
    "        # if len(preds.columns) > 0:\n",
    "        if pred.shape[1] != 0:\n",
    "            sub_part = predict_multiclass_adaptive(pred, meta_test, CFG.action_thresholds)\n",
    "            submission_list.append(sub_part)\n",
    "\n",
    "    #     del X_test\n",
    "    #     gc.collect()\n",
    "\n",
    "    # return submission_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64aacd9",
   "metadata": {
    "papermill": {
     "duration": 0.004158,
     "end_time": "2025-12-09T07:14:48.698437",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.694279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca88cf21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:14:48.707361Z",
     "iopub.status.busy": "2025-12-09T07:14:48.707166Z",
     "iopub.status.idle": "2025-12-09T08:21:26.920642Z",
     "shell.execute_reply": "2025-12-09T08:21:26.919880Z"
    },
    "papermill": {
     "duration": 3998.227016,
     "end_time": "2025-12-09T08:21:26.929431",
     "exception": false,
     "start_time": "2025-12-09T07:14:48.702415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 models from single_6.pkl\n",
      "Loaded 6 models from pair_6.pkl\n",
      "Loaded 2 models from pair_1.pkl\n",
      "Loaded 6 models from pair_0.pkl\n",
      "Loaded 1 models from single_1.pkl\n",
      "Loaded 6 models from single_5.pkl\n",
      "Loaded 1 models from single_2.pkl\n",
      "Loaded 6 models from pair_2.pkl\n",
      "Loaded 12 models from pair_5.pkl\n",
      "Loaded 17 models from pair_7.pkl\n",
      "Loaded 5 models from pair_4.pkl\n",
      "Loaded 5 models from pair_8.pkl\n",
      "Loaded 6 models from pair_3.pkl\n",
      "Loaded 4 models from single_7.pkl\n",
      "Loaded 2 models from single_8.pkl\n",
      "Loaded 1 models from single_0.pkl\n",
      "\n",
      "============================================================\n",
      "OPTUNA THRESHOLD TUNING\n",
      "============================================================\n",
      "Using 170 videos for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
      "/tmp/ipykernel_20/3930451058.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SINGLE THRESHOLDS:\n",
      "  rear                 = 0.18 (F1=0.085)\n",
      "  rest                 = 0.39 (F1=0.981)\n",
      "  selfgroom            = 0.18 (F1=0.136)\n",
      "  climb                = 0.33 (F1=0.918)\n",
      "  dig                  = 0.25 (F1=0.351)\n",
      "  run                  = 0.22 (F1=1.000)\n",
      "  huddle               = 0.27 (F1=0.254)\n",
      "  biteobject           = 0.34 (F1=0.981)\n",
      "  exploreobject        = 0.34 (F1=0.999)\n",
      "  genitalgroom         = 0.27 (F1=0.103)\n",
      "  freeze               = 0.40 (F1=0.999)\n",
      "\n",
      "PAIR THRESHOLDS:\n",
      "  sniff                = 0.40 (F1=0.731)\n",
      "  sniffgenital         = 0.34 (F1=0.568)\n",
      "  approach             = 0.35 (F1=0.491)\n",
      "  defend               = 0.38 (F1=0.796)\n",
      "  escape               = 0.38 (F1=0.683)\n",
      "  attemptmount         = 0.35 (F1=0.584)\n",
      "  reciprocalsniff      = 0.40 (F1=0.981)\n",
      "  attack               = 0.38 (F1=0.675)\n",
      "  avoid                = 0.35 (F1=0.272)\n",
      "  chase                = 0.35 (F1=0.335)\n",
      "  chaseattack          = 0.35 (F1=0.220)\n",
      "  submit               = 0.35 (F1=0.149)\n",
      "  shepherd             = 0.35 (F1=0.281)\n",
      "  flinch               = 0.35 (F1=0.285)\n",
      "  follow               = 0.38 (F1=0.767)\n",
      "  sniffface            = 0.38 (F1=0.814)\n",
      "  tussle               = 0.38 (F1=0.670)\n",
      "  disengage            = 0.34 (F1=0.971)\n",
      "  mount                = 0.40 (F1=0.660)\n",
      "  sniffbody            = 0.38 (F1=0.721)\n",
      "  intromit             = 0.40 (F1=0.914)\n",
      "  dominancemount       = 0.35 (F1=0.372)\n",
      "  allogroom            = 0.35 (F1=0.489)\n",
      "  ejaculate            = 0.25 (F1=1.000)\n",
      "  dominancegroom       = 0.37 (F1=1.000)\n",
      "\n",
      "============================================================\n",
      "COPY TO CFG:\n",
      "============================================================\n",
      "\"action_thresholds\": {\n",
      "  \"single\": {\n",
      "    \"biteobject\": 0.34,\n",
      "    \"climb\": 0.33,\n",
      "    \"dig\": 0.25,\n",
      "    \"exploreobject\": 0.34,\n",
      "    \"freeze\": 0.4,\n",
      "    \"genitalgroom\": 0.27,\n",
      "    \"huddle\": 0.27,\n",
      "    \"rear\": 0.18,\n",
      "    \"rest\": 0.39,\n",
      "    \"run\": 0.22,\n",
      "    \"selfgroom\": 0.18,\n",
      "  },\n",
      "  \"pair\": {\n",
      "    \"allogroom\": 0.35,\n",
      "    \"approach\": 0.35,\n",
      "    \"attack\": 0.38,\n",
      "    \"attemptmount\": 0.35,\n",
      "    \"avoid\": 0.35,\n",
      "    \"chase\": 0.35,\n",
      "    \"chaseattack\": 0.35,\n",
      "    \"defend\": 0.38,\n",
      "    \"disengage\": 0.34,\n",
      "    \"dominancegroom\": 0.37,\n",
      "    \"dominancemount\": 0.35,\n",
      "    \"ejaculate\": 0.25,\n",
      "    \"escape\": 0.38,\n",
      "    \"flinch\": 0.35,\n",
      "    \"follow\": 0.38,\n",
      "    \"intromit\": 0.4,\n",
      "    \"mount\": 0.4,\n",
      "    \"reciprocalsniff\": 0.4,\n",
      "    \"shepherd\": 0.35,\n",
      "    \"sniff\": 0.4,\n",
      "    \"sniffbody\": 0.38,\n",
      "    \"sniffface\": 0.38,\n",
      "    \"sniffgenital\": 0.34,\n",
      "    \"submit\": 0.35,\n",
      "    \"tussle\": 0.38,\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if CFG.mode == \"validate\": \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAIN-AND-SUBMIT PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # thresholds_all = {\"single\": {}, \"pair\": {}}\n",
    "    submission_list = []\n",
    "\n",
    "    for section in range(len(body_parts_list)):\n",
    "        # Lấy body_parts_tracked trong số 9 bộ của toàn dataset\n",
    "        body_parts_tracked_str = body_parts_list[section]\n",
    "\n",
    "        try:\n",
    "            body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "\n",
    "            if len(body_parts_tracked) > 5:\n",
    "                body_parts_tracked = [b for b in body_parts_tracked if b not in CFG.drop_body_parts]\n",
    "\n",
    "            # Lấy các rows/videos được thu với body_parts_tracked tương ứng\n",
    "            train_subset = train[train.body_parts_tracked == body_parts_tracked_str]\n",
    "\n",
    "            if train_subset.empty:\n",
    "                print(\"\\nNo videos in this section, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"SECTION {section}/{len(body_parts_list)-1} (9 sections total): {len(body_parts_tracked)} bodyparts, {len(train_subset)} videos\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "            fps_lookup = (\n",
    "                train_subset[[\"video_id\", \"frames_per_second\"]]\n",
    "                .drop_duplicates(\"video_id\")\n",
    "                .set_index(\"video_id\")[\"frames_per_second\"]\n",
    "                .to_dict()\n",
    "            )\n",
    "    \n",
    "            single_mouse = []\n",
    "            single_meta = []\n",
    "            single_y = []\n",
    "\n",
    "            pair_mouse = []\n",
    "            pair_meta = []\n",
    "            pair_y = []\n",
    "\n",
    "            # Accumulate generated data\n",
    "            for mode, data, meta, labels in generate_mouse_data(train_subset, mode=\"train\"):\n",
    "                video_id = meta[\"video_id\"].iloc[0]\n",
    "                fps = fps_lookup.get(video_id, 30.0)\n",
    "\n",
    "                if mode == \"single\":\n",
    "                    single_mouse.append(data)\n",
    "                    single_meta.append(meta)\n",
    "                    single_y.append(labels)\n",
    "\n",
    "                else:\n",
    "                    pair_mouse.append(data)\n",
    "                    pair_meta.append(meta)\n",
    "                    pair_y.append(labels)\n",
    "\n",
    "            # Single models\n",
    "            if len(single_mouse) > 0:\n",
    "                print(f\"Processing {len(single_mouse)} single mouse videos...\")\n",
    "                single_X = []\n",
    "\n",
    "                for data_i, meta_i, in zip(single_mouse, single_meta):\n",
    "                    fps_i = _fps_from_meta(meta_i, fps_lookup, default_fps=30.0)\n",
    "\n",
    "                    X_i = build_single_features(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                    single_X.append(X_i)\n",
    "        \n",
    "                X_all = pd.concat(single_X, ignore_index=True)\n",
    "                y_all = pd.concat(single_y, ignore_index=True)\n",
    "                meta_all = pd.concat(single_meta, ignore_index=True)\n",
    "        \n",
    "                print(f\"Shape: {X_all.shape[0]} frames × {X_all.shape[1]} features\")\n",
    "        \n",
    "                # Train ENSEMBLE + ADAPTIVE THRESHOLDING và thực hiện predict\n",
    "                train_models_and_submit_ensemble(section, body_parts_tracked_str, \"single\", X_all, y_all, meta_all, 2_000_000)\n",
    "\n",
    "                del X_all, y_all, meta_all, single_X, single_mouse, single_meta, single_y\n",
    "                gc.collect()\n",
    "\n",
    "            # Train pair models\n",
    "            if len(pair_mouse) > 0:\n",
    "                print(f\"Processing {len(pair_mouse)} pair mouse videos...\")\n",
    "                pair_X = []\n",
    "            \n",
    "                for data_i, meta_i in zip(pair_mouse, pair_meta):\n",
    "                    fps_i = _fps_from_meta(meta_i, fps_lookup, default_fps=30.0)\n",
    "\n",
    "                    X_i = build_pair_features(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                    pair_X.append(X_i)   \n",
    "\n",
    "                X_all = pd.concat(pair_X, ignore_index=True)\n",
    "                y_all = pd.concat(pair_y, ignore_index=True)\n",
    "                meta_all = pd.concat(pair_meta, ignore_index=True)  \n",
    "        \n",
    "                print(f\"Shape: {X_all.shape[0]} frames × {X_all.shape[1]} features\")\n",
    "        \n",
    "                # Train ENSEMBLE + ADAPTIVE THRESHOLDING + thực hiện predict\n",
    "                train_models_and_submit_ensemble(section, body_parts_tracked_str, \"pair\", X_all, y_all, meta_all, 900_000)\n",
    "\n",
    "                del X_all, y_all, meta_all, pair_X, pair_mouse, pair_y, pair_meta\n",
    "                gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"***Exception*** {str(e)[:100]}\")\n",
    "\n",
    "    # Save thresholds\n",
    "    # joblib.dump(thresholds_all, f\"{CFG.model_save_dir}/thresholds.pkl\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Training complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "    # Tạo final submission\n",
    "    if len(submission_list) > 0:\n",
    "        submission = pd.concat(submission_list, ignore_index=True)\n",
    "    else:\n",
    "        # Empty fallback\n",
    "        print(\"WARNING: No predictions generated!\")\n",
    "        submission = pd.DataFrame({\n",
    "            \"video_id\": [], \"agent_id\": [], \"target_id\": [],\n",
    "            \"action\": [], \"start_frame\": [], \"stop_frame\": []\n",
    "        })\n",
    "\n",
    "    # Làm sạch submission\n",
    "    submission = clean_submission(submission, test_csv, \"test\", CFG.test_tracking_path)\n",
    "    \n",
    "    # Thêm row_id\n",
    "    submission.insert(0, \"row_id\", range(len(submission)))\n",
    "    \n",
    "    # Save\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUBMISSION COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total events: {len(submission):,}\")\n",
    "    print(f\"Unique videos: {submission.video_id.nunique()}\")\n",
    "    print(f\"Actions: {submission.action.value_counts().to_dict()}\")\n",
    "    print(f\"Saved to: submission.csv\")\n",
    "\n",
    "\n",
    "elif CFG.mode == \"tune\":\n",
    "    # Load saved models\n",
    "    all_models = load_all_models(CFG.model_tune_dir)\n",
    "\n",
    "    # Run Optuna tuning\n",
    "    tuned_thresholds = run_threshold_tuning_optuna(\n",
    "        train_csv, \n",
    "        all_models, \n",
    "        n_trials=100  # Takes ~5 min for all actions\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "isSourceIdPinned": false,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8870798,
     "sourceId": 13944162,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8908235,
     "sourceId": 14069739,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4016.666363,
   "end_time": "2025-12-09T08:21:29.459215",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-09T07:14:32.792852",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
