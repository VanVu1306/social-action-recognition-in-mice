{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff67792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:09.828185Z",
     "iopub.status.busy": "2025-12-01T00:34:09.827941Z",
     "iopub.status.idle": "2025-12-01T00:34:27.658599Z",
     "shell.execute_reply": "2025-12-01T00:34:27.657722Z"
    },
    "papermill": {
     "duration": 17.837555,
     "end_time": "2025-12-01T00:34:27.659897",
     "exception": false,
     "start_time": "2025-12-01T00:34:09.822342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HARDWARE CHECK\n",
      "============================================================\n",
      "GPU Available: True\n",
      "GPU Name: Tesla T4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json, ast\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os, math\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Hardware check\n",
    "import torch\n",
    "print(\"=\"*60)\n",
    "print(\"HARDWARE CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00786f2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:27.669197Z",
     "iopub.status.busy": "2025-12-01T00:34:27.668447Z",
     "iopub.status.idle": "2025-12-01T00:34:27.673192Z",
     "shell.execute_reply": "2025-12-01T00:34:27.672490Z"
    },
    "papermill": {
     "duration": 0.010156,
     "end_time": "2025-12-01T00:34:27.674287",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.664131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # mode = \"validate\"\n",
    "    mode = \"submit\"\n",
    "\n",
    "    if mode == \"validate\":\n",
    "        model_save_dir = \"/kaggle/working\"\n",
    "    else:\n",
    "        model_save_dir = \"/kaggle/input/lgbm-model-mabe-results\"\n",
    "\n",
    "    train_csv_path = \"/kaggle/input/MABe-mouse-behavior-detection/train.csv\" \n",
    "    test_csv_path = \"/kaggle/input/MABe-mouse-behavior-detection/test.csv\"\n",
    "    train_annotation_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\"\n",
    "    train_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\"\n",
    "    test_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/test_tracking\"\n",
    "\n",
    "    drop_body_parts =  [\n",
    "        'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "        'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "        'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8264fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:27.682431Z",
     "iopub.status.busy": "2025-12-01T00:34:27.682223Z",
     "iopub.status.idle": "2025-12-01T00:34:27.823157Z",
     "shell.execute_reply": "2025-12-01T00:34:27.822460Z"
    },
    "papermill": {
     "duration": 0.146432,
     "end_time": "2025-12-01T00:34:27.824449",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.678017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(CFG.train_csv_path) \n",
    "test_csv = pd.read_csv(CFG.test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6c0447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:27.833359Z",
     "iopub.status.busy": "2025-12-01T00:34:27.832945Z",
     "iopub.status.idle": "2025-12-01T00:34:27.862136Z",
     "shell.execute_reply": "2025-12-01T00:34:27.861532Z"
    },
    "papermill": {
     "duration": 0.034859,
     "end_time": "2025-12-01T00:34:27.863254",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.828395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_lab = train_csv[\"lab_id\"].str.startswith(\"MABe22\")\n",
    "mask_behavior = train_csv[\"behaviors_labeled\"].isna() | (train_csv[\"behaviors_labeled\"].str.strip() == \"\")\n",
    "mask_drop = mask_lab | mask_behavior\n",
    "\n",
    "train = train_csv[~mask_drop]\n",
    "body_parts_list = list(np.unique(train.body_parts_tracked))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cee966",
   "metadata": {
    "papermill": {
     "duration": 0.003427,
     "end_time": "2025-12-01T00:34:27.870388",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.866961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680375ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:27.878407Z",
     "iopub.status.busy": "2025-12-01T00:34:27.878199Z",
     "iopub.status.idle": "2025-12-01T00:34:27.892029Z",
     "shell.execute_reply": "2025-12-01T00:34:27.891470Z"
    },
    "papermill": {
     "duration": 0.019187,
     "end_time": "2025-12-01T00:34:27.893057",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.873870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_mouse_data(datasubset, mode, traintest_directory=None, generate_single=True, generate_pair=True):\n",
    "    \"\"\"\n",
    "    Yields:\n",
    "        (mode, X, meta, y)\n",
    "        mode: \"single\" hoặc \"pair\"\n",
    "        X: raw features DataFrame\n",
    "        meta: metadata DataFrame\n",
    "        y: labels (đối với train mode) hoặc action list (đối với test mode)\n",
    "    \"\"\"\n",
    "\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{mode}_tracking\"\n",
    "\n",
    "    for idx, row in datasubset.iterrows():\n",
    "        lab_id = row.lab_id\n",
    "        video_id = row.video_id\n",
    "        pix_per_cm = row.pix_per_cm_approx\n",
    "        fps = row.frames_per_second\n",
    "\n",
    "        # Bỏ qua MABe22 labs hoặc missing behaviors\n",
    "        if lab_id.startswith(\"MABe22\"):\n",
    "            continue\n",
    "        if mode == \"train\" and (pd.isna(row.behaviors_labeled) or str(row.behaviors_labeled).strip() == \"\"):\n",
    "            continue\n",
    "\n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "\n",
    "        # Load tracking\n",
    "        try:\n",
    "            vid = pd.read_parquet(path)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "        # Bỏ bớt bodyparts\n",
    "        if len(np.unique(vid.bodypart)) > 5:\n",
    "            vid = vid[~vid.bodypart.isin(CFG.drop_body_parts)]\n",
    "\n",
    "        pvid = vid.pivot(\n",
    "            index=\"video_frame\",\n",
    "            columns=[\"mouse_id\", \"bodypart\"],\n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n",
    "        pvid /= pix_per_cm\n",
    "\n",
    "        del vid\n",
    "        gc.collect()\n",
    "\n",
    "        mouse_ids = pvid.columns.get_level_values(0).unique().tolist()\n",
    "\n",
    "        # Tìm behaviors tracked trong CSV file\n",
    "        vid_behaviors = json.loads(row.behaviors_labeled)\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=[\"agent\", \"target\", \"action\"])\n",
    "\n",
    "        # Load annotation (đối với training mode)\n",
    "        if mode == \"train\":\n",
    "            try: \n",
    "                annot = pd.read_parquet(path.replace(\"train_tracking\", \"train_annotation\"))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        else:\n",
    "            annot = None\n",
    "\n",
    "\n",
    "        # Build data cho single mouse \n",
    "        if generate_single:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\")\n",
    "\n",
    "            for mouse_id_str in vid_behaviors_subset.agent.unique():\n",
    "                try:\n",
    "                    mouse_id = int(mouse_id_str.replace(\"mouse\", \"\"))\n",
    "                \n",
    "                    if mouse_id not in mouse_ids:\n",
    "                        continue\n",
    "\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n",
    "\n",
    "                    # Single mouse raw features - toạ độ bodyparts\n",
    "                    single_mouse = pvid.loc[:, mouse_id]\n",
    "                    assert len(single_mouse) == len(pvid)\n",
    "                \n",
    "                    # Single mouse meta data\n",
    "                    meta = pd.DataFrame({\n",
    "                        \"video_id\": video_id,\n",
    "                        \"agent_id\": mouse_id_str,\n",
    "                        \"target_id\": \"self\",\n",
    "                        \"video_frame\": single_mouse.index,\n",
    "                    })\n",
    "\n",
    "                    # Single mouse labels\n",
    "                    if mode == \"train\":\n",
    "                        labels = pd.DataFrame(0.0, index=single_mouse.index, columns=vid_agent_actions)\n",
    "                    \n",
    "                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n",
    "\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            labels.loc[annot_row[\"start_frame\"]:annot_row[\"stop_frame\"], annot_row.action] = 1.0\n",
    "                        yield \"single\", single_mouse, meta, labels\n",
    "                    else:\n",
    "                        yield \"single\", single_mouse, meta, vid_agent_actions\n",
    "\n",
    "                except (KeyError, ValueError):\n",
    "                    pass\n",
    "\n",
    "        # Build data cho mouse pair\n",
    "        if generate_pair:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n",
    "\n",
    "            if len(vid_behaviors_subset) > 0:\n",
    "                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values(\"mouse_id\")), 2):\n",
    "                    agent_str = f\"mouse{agent}\"\n",
    "                    target_str = f\"mouse{target}\"\n",
    "\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n",
    "\n",
    "                    if len(vid_agent_actions) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Mouse pair raw features - toạ độ bodyparts của cặp chuột\n",
    "                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=[\"A\", \"B\"])  # Raw coordinates\n",
    "                    assert len(mouse_pair) == len(pvid)\n",
    "\n",
    "                    # Mouse pair meta data\n",
    "                    meta = pd.DataFrame({\n",
    "                        \"video_id\": video_id,\n",
    "                        \"agent_id\": agent_str,\n",
    "                        \"target_id\": target_str,\n",
    "                        \"video_frame\": pvid.index,\n",
    "                    })\n",
    "\n",
    "                    # Mouse pair labels\n",
    "                    if mode == \"train\":\n",
    "                        labels = pd.DataFrame(0.0, index=pvid.index, columns=vid_agent_actions)\n",
    "                    \n",
    "                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n",
    "\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            labels.loc[annot_row[\"start_frame\"]:annot_row[\"stop_frame\"], annot_row.action] = 1.0\n",
    "                        yield \"pair\", mouse_pair, meta, labels\n",
    "                    else:\n",
    "                        yield \"pair\", mouse_pair, meta, vid_agent_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34a11b",
   "metadata": {
    "papermill": {
     "duration": 0.003352,
     "end_time": "2025-12-01T00:34:27.900136",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.896784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28d0d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:27.907845Z",
     "iopub.status.busy": "2025-12-01T00:34:27.907476Z",
     "iopub.status.idle": "2025-12-01T00:34:27.911185Z",
     "shell.execute_reply": "2025-12-01T00:34:27.910710Z"
    },
    "papermill": {
     "duration": 0.008689,
     "end_time": "2025-12-01T00:34:27.912193",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.903504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _fps_from_meta(meta_df, fallback_lookup, default_fps=30.0):\n",
    "    \"\"\"Get FPS with proper fallback chain\"\"\"\n",
    "    if \"frames_per_second\" in meta_df.columns:\n",
    "        fps_val = meta_df[\"frames_per_second\"].iloc[0]\n",
    "        if pd.notnull(fps_val) and fps_val > 0:\n",
    "            return float(fps_val)\n",
    "    \n",
    "    vid = meta_df[\"video_id\"].iloc[0]\n",
    "    if vid in fallback_lookup:\n",
    "        return float(fallback_lookup[vid])\n",
    "    \n",
    "    return default_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be30898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:27.920312Z",
     "iopub.status.busy": "2025-12-01T00:34:27.920098Z",
     "iopub.status.idle": "2025-12-01T00:34:27.932611Z",
     "shell.execute_reply": "2025-12-01T00:34:27.931916Z"
    },
    "papermill": {
     "duration": 0.01793,
     "end_time": "2025-12-01T00:34:27.933659",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.915729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_centers(df):\n",
    "    \"\"\"\n",
    "    Đảm bảo \"body_center\" tồn tại với mọi chuột hoặc bodyparts combination.\n",
    "    Xử lý cả cột 2-level (bodypart, coord) (từng chuột) và 3-level (mouse_id, bodypart, coord) (cặp chuột).\n",
    "    \n",
    "    Fallback logic:\n",
    "    1. Nếu nose và tail_base tồn tại → midpoint(nose, tail_base)\n",
    "    2. Else if head và tail_base tồn tại → midpoint(head, tail_base)\n",
    "    3. Else if chỉ tail_base tồn tại → use tail_base\n",
    "    4. Else → không tính được body_center\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "\n",
    "    # Cột 2-level (bodypart, coord)\n",
    "    if cols.nlevels == 2:\n",
    "        if (\"body_center\", \"x\") not in df.columns or (\"body_center\", \"y\") not in df.columns:\n",
    "            if (\"nose\", \"x\") in df.columns and (\"tail_base\", \"x\") in df.columns:\n",
    "                df[(\"body_center\", \"x\")] = (df[(\"nose\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
    "                df[(\"body_center\", \"y\")] = (df[(\"nose\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
    "            elif (\"head\", \"x\") in df.columns and (\"tail_base\", \"x\") in df.columns:\n",
    "                df[(\"body_center\", \"x\")] = (df[(\"head\", \"x\")] + df[(\"tail_base\", \"x\")]) / 2\n",
    "                df[(\"body_center\", \"y\")] = (df[(\"head\", \"y\")] + df[(\"tail_base\", \"y\")]) / 2\n",
    "            elif (\"tail_base\", \"x\") in df.columns:\n",
    "                df[(\"body_center\", \"x\")] = df[(\"tail_base\", \"x\")]\n",
    "                df[(\"body_center\", \"y\")] = df[(\"tail_base\", \"y\")]\n",
    "            else:\n",
    "                # no valid bodyparts → fill NaN\n",
    "                df[(\"body_center\", \"x\")] = np.nan\n",
    "                df[(\"body_center\", \"y\")] = np.nan\n",
    "\n",
    "    # Cột 3-level (mouse_id, bodypart, coord)\n",
    "    elif cols.nlevels == 3:\n",
    "        mice = sorted(list(set(c[0] for c in cols)))\n",
    "\n",
    "        for m in mice:\n",
    "            has_body_center = ((m, \"body_center\", \"x\") in cols) and ((m, \"body_center\", \"y\") in cols)\n",
    "            if not has_body_center:\n",
    "                if ((m, \"nose\", \"x\") in cols) and ((m, \"tail_base\", \"x\") in cols):\n",
    "                    df[(m, \"body_center\", \"x\")] = (df[(m, \"nose\", \"x\")] + df[(m, \"tail_base\", \"x\")]) / 2\n",
    "                    df[(m, \"body_center\", \"y\")] = (df[(m, \"nose\", \"y\")] + df[(m, \"tail_base\", \"y\")]) / 2\n",
    "                elif ((m, \"head\", \"x\") in cols) and ((m, \"tail_base\", \"x\") in cols):\n",
    "                    df[(m, \"body_center\", \"x\")] = (df[(m, \"head\", \"x\")] + df[(m, \"tail_base\", \"x\")]) / 2\n",
    "                    df[(m, \"body_center\", \"y\")] = (df[(m, \"head\", \"y\")] + df[(m, \"tail_base\", \"y\")]) / 2\n",
    "                elif ((m, \"tail_base\", \"x\") in cols):\n",
    "                    df[(m, \"body_center\", \"x\")] = df[(m, \"tail_base\", \"x\")]\n",
    "                    df[(m, \"body_center\", \"y\")] = df[(m, \"tail_base\", \"y\")]\n",
    "                else:\n",
    "                    df[(m, \"body_center\", \"x\")] = np.nan\n",
    "                    df[(m, \"body_center\", \"y\")] = np.nan\n",
    "    return df\n",
    "\n",
    "def calculate_speed_lag(df, part, fps, lag=10, mouse=None):\n",
    "    cols = df.columns\n",
    "    if mouse is not None:\n",
    "        x = df[(mouse, part, \"x\")]\n",
    "        y = df[(mouse, part, \"y\")]\n",
    "    else:\n",
    "        x = df[(part, \"x\")]\n",
    "        y = df[(part, \"y\")]\n",
    "\n",
    "    if x.isna().all() or y.isna().all():\n",
    "        # all missing → return zeros\n",
    "        return pd.Series(0, index=df.index)\n",
    "\n",
    "    dx = x.diff(lag)\n",
    "    dy = y.diff(lag)\n",
    "    speed = np.sqrt(dx**2 + dy**2) * fps\n",
    "    return speed.fillna(0)\n",
    "\n",
    "# Tính các thống kê của 1 đại lượng theo nhiều cửa sổ thời gian\n",
    "def calculate_window_stats(df, metric, name, fps, scales=[60, 90, 120]):\n",
    "    \"\"\"\n",
    "    Thêm rolling statistics cho bất kỳ series nào.\n",
    "    \n",
    "    metric : pd.Series (ví dụ speed, distance, curvature...)\n",
    "    fps    : frames_per_second\n",
    "    scales : list window sizes quy đổi theo 30fps → mặc định [60, 90, 120]\n",
    "    \"\"\"\n",
    "    res = pd.DataFrame(index=df.index)\n",
    "    for scale in scales:\n",
    "        ws = max(1, int(round(scale * float(fps) / 30)))\n",
    "        roll = metric.rolling(ws, min_periods=max(1, ws//4))\n",
    "\n",
    "        res[f\"{name}_mean_{scale}\"] = roll.mean()\n",
    "        res[f\"{name}_std_{scale}\"]  = roll.std()\n",
    "        res[f\"{name}_min_{scale}\"]  = roll.min()\n",
    "        res[f\"{name}_max_{scale}\"]  = roll.max()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d118a5b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:27.941737Z",
     "iopub.status.busy": "2025-12-01T00:34:27.941393Z",
     "iopub.status.idle": "2025-12-01T00:34:27.961945Z",
     "shell.execute_reply": "2025-12-01T00:34:27.961230Z"
    },
    "papermill": {
     "duration": 0.026035,
     "end_time": "2025-12-01T00:34:27.963015",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.936980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_single_features(single_mouse_df, body_parts_tracked, meta_fps):\n",
    "    single_mouse_df = calculate_centers(single_mouse_df)\n",
    "\n",
    "    # Get actual bodypart columns\n",
    "    available_body_parts = single_mouse_df.columns.get_level_values(0).unique()\n",
    "    \n",
    "    # === Shape and Position Features ===\n",
    "    # Euclidean distances giữa các cặp bodyparts\n",
    "    X = pd.DataFrame({\n",
    "        f\"{p1}+{p2}\": np.sqrt(\n",
    "            (single_mouse_df[(p1, \"x\")] - single_mouse_df[(p2, \"x\")])**2 + \n",
    "            (single_mouse_df[(p1, \"y\")] - single_mouse_df[(p2, \"y\")])**2\n",
    "        )\n",
    "        for p1, p2 in itertools.combinations(body_parts_tracked, 2)\n",
    "        if p1 in available_body_parts and p2 in available_body_parts\n",
    "    })\n",
    "    \n",
    "    expected_cols = [f\"{p1}+{p2}\" for p1, p2 in itertools.combinations(body_parts_tracked, 2)]\n",
    "    X = X.reindex(columns=expected_cols, copy=False)\n",
    "    \n",
    "    # Elongation (chỉ khi required bodyparts tồn tại)\n",
    "    if \"nose\" in available_body_parts and \"tail_base\" in available_body_parts and \"ear_left\" in available_body_parts and \"ear_right\" in available_body_parts:\n",
    "        X[\"elong\"] = X[\"nose+tail_base\"] / (X[\"ear_left+ear_right\"] + 1e-6)\n",
    "    else:\n",
    "        X[\"elong\"] = 0.0\n",
    "    \n",
    "    # Body angle (chỉ khi nose, tail_base, body_center tồn tại)\n",
    "    if all(bp in available_body_parts for bp in [\"nose\", \"tail_base\", \"body_center\"]):\n",
    "        v1_x = single_mouse_df[(\"nose\",\"x\")] - single_mouse_df[(\"body_center\",\"x\")]\n",
    "        v1_y = single_mouse_df[(\"nose\",\"y\")] - single_mouse_df[(\"body_center\",\"y\")]\n",
    "        v2_x = single_mouse_df[(\"tail_base\",\"x\")] - single_mouse_df[(\"body_center\",\"x\")]\n",
    "        v2_y = single_mouse_df[(\"tail_base\",\"y\")] - single_mouse_df[(\"body_center\",\"y\")]\n",
    "        X[\"body_angle\"] = (v1_x*v2_x + v1_y*v2_y) / (np.sqrt(v1_x**2+v1_y**2) * np.sqrt(v2_x**2+v2_y**2) + 1e-6)\n",
    "    else:\n",
    "        X[\"body_angle\"] = 0.0\n",
    "    \n",
    "    # === Movement Features ===\n",
    "    if \"body_center\" in available_body_parts:\n",
    "        X[\"speed\"] = np.sqrt(\n",
    "            single_mouse_df[(\"body_center\", \"x\")].diff()**2 +\n",
    "            single_mouse_df[(\"body_center\", \"y\")].diff()**2\n",
    "        )\n",
    "        X[\"accelerate\"] = X[\"speed\"].diff()\n",
    "    else:\n",
    "        X[\"speed\"] = 0.0\n",
    "        X[\"accelerate\"] = 0.0\n",
    "    \n",
    "    # For available bodyparts\n",
    "    for p in [\"body_center\", \"ear_left\", \"ear_right\"]:\n",
    "        if p in available_body_parts:\n",
    "            # Speed lag features\n",
    "            X[f\"speed_{p}_lag_10\"] = calculate_speed_lag(single_mouse_df, p, fps=meta_fps)\n",
    "\n",
    "            # Rolling stats\n",
    "            speed = np.sqrt(single_mouse_df[(p, \"x\")].diff()**2 + single_mouse_df[(p, \"y\")].diff()**2) * float(meta_fps)\n",
    "            res = calculate_window_stats(single_mouse_df, speed, f\"speed_{p}\", fps=meta_fps)\n",
    "            X = pd.concat([X, res], axis=1)\n",
    "\n",
    "            # Curvature\n",
    "            lag = max(1, int(round(10 * float(meta_fps) / 30)))\n",
    "            shifted_x = single_mouse_df[(p,\"x\")].shift(lag)\n",
    "            shifted_y = single_mouse_df[(p,\"y\")].shift(lag)\n",
    "            X[f\"curvature_{p}_lag_{lag}\"] = np.sqrt((shifted_x - single_mouse_df[(p,\"x\")])**2 + \n",
    "                                                              (shifted_y - single_mouse_df[(p,\"y\")])**2)\n",
    "        else:\n",
    "            # Dummy columns\n",
    "            X[f\"speed_{p}_lag_10\"] = 0.0\n",
    "            \n",
    "            for scale in [40, 60, 80]:\n",
    "                X[f\"speed_{p}_mean_{scale}\"] = 0.0\n",
    "                X[f\"speed_{p}_std_{scale}\"] = 0.0\n",
    "                X[f\"speed_{p}_min_{scale}\"] = 0.0\n",
    "                X[f\"speed_{p}_max_{scale}\"] = 0.0\n",
    "                \n",
    "            lag = max(1, int(round(10 * float(meta_fps) / 30)))\n",
    "            X[f\"curvature_{p}_lag_{lag}\"] = 0.0\n",
    "\n",
    "    X = X.T.drop_duplicates().T\n",
    "    \n",
    "    return X.astype(np.float32, copy=False).fillna(0)\n",
    "\n",
    "\n",
    "def build_pair_features(mouse_pair_df, body_parts_tracked, meta_fps):\n",
    "    mouse_pair_df = calculate_centers(mouse_pair_df)\n",
    "    \n",
    "    # Get bodyparts for both mice\n",
    "    avail_A = mouse_pair_df[\"A\"].columns.get_level_values(0).unique()\n",
    "    avail_B = mouse_pair_df[\"B\"].columns.get_level_values(0).unique()\n",
    "    \n",
    "    # Pairwise distances\n",
    "    X = pd.DataFrame({\n",
    "        f\"A_{p1}+B_{p2}\": np.sqrt(\n",
    "            (mouse_pair_df[(\"A\", p1, \"x\")] - mouse_pair_df[(\"B\", p2, \"x\")])**2 +\n",
    "            (mouse_pair_df[(\"A\", p1, \"y\")] - mouse_pair_df[(\"B\", p2, \"y\")])**2\n",
    "        )\n",
    "        for p1, p2 in itertools.product(body_parts_tracked, repeat=2)\n",
    "        if p1 in avail_A and p2 in avail_B\n",
    "    })\n",
    "    \n",
    "    expected_cols = [f\"A_{p1}+B_{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)]\n",
    "    X = X.reindex(columns=expected_cols, copy=False)\n",
    "    \n",
    "    # Relative orientation (chỉ khi nose (không có nose thì head) và tail_base tồn tại ở cả 2 chuột)\n",
    "    if all(bp in avail_A for bp in [\"nose\",\"tail_base\"]) and all(bp in avail_B for bp in [\"nose\",\"tail_base\"]):\n",
    "        vec_A_x = mouse_pair_df[(\"A\", \"nose\", \"x\")] - mouse_pair_df[(\"A\", \"tail_base\", \"x\")]\n",
    "        vec_A_y = mouse_pair_df[(\"A\", \"nose\", \"y\")] - mouse_pair_df[(\"A\", \"tail_base\", \"y\")]\n",
    "        vec_B_x = mouse_pair_df[(\"B\", \"nose\", \"x\")] - mouse_pair_df[(\"B\", \"tail_base\", \"x\")]\n",
    "        vec_B_y = mouse_pair_df[(\"B\", \"nose\", \"y\")] - mouse_pair_df[(\"B\", \"tail_base\", \"y\")]\n",
    "        X[\"relative_orientation\"] = (vec_A_x*vec_B_x + vec_A_y*vec_B_y) / (\n",
    "            np.sqrt(vec_A_x**2 + vec_A_y**2) * np.sqrt(vec_B_x**2 + vec_B_y**2) + 1e-6\n",
    "        )\n",
    "    elif all(bp in avail_A for bp in [\"head\",\"tail_base\"]) and all(bp in avail_B for bp in [\"head\",\"tail_base\"]):\n",
    "        vec_A_x = mouse_pair_df[(\"A\", \"head\", \"x\")] - mouse_pair_df[(\"A\", \"tail_base\", \"x\")]\n",
    "        vec_A_y = mouse_pair_df[(\"A\", \"head\", \"y\")] - mouse_pair_df[(\"A\", \"tail_base\", \"y\")]\n",
    "        vec_B_x = mouse_pair_df[(\"B\", \"head\", \"x\")] - mouse_pair_df[(\"B\", \"tail_base\", \"x\")]\n",
    "        vec_B_y = mouse_pair_df[(\"B\", \"head\", \"y\")] - mouse_pair_df[(\"B\", \"tail_base\", \"y\")]\n",
    "        X[\"relative_orientation\"] = (vec_A_x*vec_B_x + vec_A_y*vec_B_y) / (\n",
    "            np.sqrt(vec_A_x**2 + vec_A_y**2) * np.sqrt(vec_B_x**2 + vec_B_y**2) + 1e-6\n",
    "        )\n",
    "    else:\n",
    "        X[\"relative_orientation\"] = 0.0\n",
    "    \n",
    "    # Các đại lượng dựa trên body_center\n",
    "    if \"body_center\" in avail_A and \"body_center\" in avail_B:\n",
    "        dist_center = np.sqrt(\n",
    "            (mouse_pair_df[(\"A\", \"body_center\", \"x\")] - mouse_pair_df[(\"B\", \"body_center\", \"x\")])**2 +\n",
    "            (mouse_pair_df[(\"A\", \"body_center\", \"y\")] - mouse_pair_df[(\"B\", \"body_center\", \"y\")])**2\n",
    "        )\n",
    "\n",
    "        # Approach\n",
    "        approach = dist_center.diff().fillna(0)\n",
    "        X[\"approach_A\"] = approach\n",
    "        X[\"approach_B\"] = approach\n",
    "        \n",
    "        # Relative distance stats\n",
    "        res = calculate_window_stats(mouse_pair_df, dist_center**2, \"center_distance\", fps=meta_fps)\n",
    "        X = pd.concat([X, res], axis=1)\n",
    "        \n",
    "        # Relative speed\n",
    "        speed_A = calculate_speed_lag(mouse_pair_df, \"body_center\", meta_fps, mouse=\"A\")\n",
    "        speed_B = calculate_speed_lag(mouse_pair_df, \"body_center\", meta_fps, mouse=\"B\")\n",
    "        X[\"speed_A_lag_10\"] = speed_A\n",
    "        X[\"speed_B_lag_10\"] = speed_B\n",
    "        X[\"relative_speed_A_B_lag_10\"] = (speed_A - speed_B).abs()\n",
    "\n",
    "        # Ngưỡng khoảng cách\n",
    "        thresholds = {\n",
    "            \"very_close\": 20,\n",
    "            \"close\": 40,\n",
    "            \"medium\": 60\n",
    "        }\n",
    "        X[\"very_close\"] = (dist_center < thresholds[\"very_close\"]).astype(float)\n",
    "        X[\"close\"] = ((dist_center >= thresholds[\"very_close\"]) & (dist_center < thresholds[\"close\"])).astype(float)\n",
    "        X[\"medium\"] = ((dist_center >= thresholds[\"close\"]) & (dist_center < thresholds[\"medium\"])).astype(float)\n",
    "        X[\"far\"] = (dist_center >= thresholds[\"medium\"]).astype(float)\n",
    "\n",
    "    else:\n",
    "        # Dummy columns\n",
    "        X[\"approach_A\"] = 0.0\n",
    "        X[\"approach_B\"] = 0.0\n",
    "        \n",
    "        for scale in [40, 60, 80]:\n",
    "            X[f\"center_distance_mean_{scale}\"] = 0.0\n",
    "            X[f\"center_distance_std_{scale}\"] = 0.0\n",
    "            X[f\"center_distance_min_{scale}\"] = 0.0\n",
    "            X[f\"center_distance_max_{scale}\"] = 0.0\n",
    "        \n",
    "        X[\"speed_A_lag_10\"] = 0.0\n",
    "        X[\"speed_B_lag_10\"] = 0.0\n",
    "        X[\"relative_speed_A_B_lag_10\"] = 0.0\n",
    "\n",
    "        X[\"very_close\"] = 0.0\n",
    "        X[\"close\"] = 0.0\n",
    "        X[\"medium\"] = 0.0\n",
    "        X[\"far\"] = 0.0\n",
    "\n",
    "    X = X.T.drop_duplicates().T\n",
    "    \n",
    "    return X.astype(np.float32, copy=False).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7e32c",
   "metadata": {
    "papermill": {
     "duration": 0.003401,
     "end_time": "2025-12-01T00:34:27.969902",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.966501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5670d40d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:27.977739Z",
     "iopub.status.busy": "2025-12-01T00:34:27.977509Z",
     "iopub.status.idle": "2025-12-01T00:34:27.988108Z",
     "shell.execute_reply": "2025-12-01T00:34:27.987599Z"
    },
    "papermill": {
     "duration": 0.015937,
     "end_time": "2025-12-01T00:34:27.989128",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.973191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_action_models(X_all, y_all, meta_all, section_idx, mode, n_splits=3):\n",
    "    \"\"\"\n",
    "    Train binary classifiers cho từng action với cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "        f1_scores: list các tuple (section, mode, action, f1_score)\n",
    "        thresholds: dict mapping action -> optimal threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    save_dir = f\"/kaggle/working/models/{section_idx}/{mode}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    f1_scores = []\n",
    "    thresholds = {}\n",
    "    actions = y_all.columns\n",
    "    \n",
    "    print(f\"\\nTraining {len(actions)} action classifiers...\")\n",
    "    \n",
    "    for action in actions:\n",
    "        print(f\"\\n[{mode.upper()} | {action}]\", end=\" \")\n",
    "        \n",
    "        # Lấy các frame được labeled là action này\n",
    "        action_mask = ~y_all[action].isna()\n",
    "        if action_mask.sum() == 0:\n",
    "            print(\"No labels, skipping\")\n",
    "            continue\n",
    "        \n",
    "        y = y_all[action][action_mask].values.astype(np.int8)\n",
    "        X = X_all[action_mask].values.astype(np.float32)\n",
    "        groups = meta_all[\"video_id\"][action_mask].values\n",
    "        \n",
    "        if y.sum() == 0:\n",
    "            print(\"-> No positive samples, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Kiểm tra có đủ groups cho CV không\n",
    "        unique_groups = np.unique(groups)\n",
    "        n_groups = len(unique_groups)\n",
    "        \n",
    "        if n_groups < 2:\n",
    "            print(f\"-> Only {n_groups} video, training without CV\")\n",
    "            \n",
    "            # Train without CV\n",
    "            clf = LGBMClassifier(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=5,\n",
    "                num_leaves=31, subsample=0.8, \n",
    "                min_child_samples=20, reg_alpha=0.1, reg_lambda=0.1, force_col_wise=True,\n",
    "                random_state=42, n_jobs=-1, verbose=-1\n",
    "            )\n",
    "            clf.fit(X, y)\n",
    "            \n",
    "            joblib.dump(clf, f\"{save_dir}/{action}_model.pkl\")\n",
    "            joblib.dump(0.5, f\"{save_dir}/{action}_threshold.pkl\")\n",
    "            \n",
    "            thresholds[action] = 0.5\n",
    "            f1_scores.append((section_idx, mode, action, -1.0))\n",
    "            \n",
    "            del clf\n",
    "            gc.collect()\n",
    "            continue\n",
    "        \n",
    "        # Cross-validation\n",
    "        n_splits_use = min(n_splits, n_groups)\n",
    "        print(f\"-> CV with {n_splits_use} folds\", end=\" \")\n",
    "        \n",
    "        gkf = StratifiedKFold(n_splits=n_splits_use, shuffle=True)\n",
    "        oof_preds = np.zeros(len(y), dtype=np.float32)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "            clf = LGBMClassifier(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=5,\n",
    "                num_leaves=31, subsample=0.8,\n",
    "                min_child_samples=20, reg_alpha=0.1, reg_lambda=0.1, force_col_wise=True,\n",
    "                random_state=42, n_jobs=-1, verbose=-1\n",
    "            )\n",
    "            clf.fit(X[train_idx], y[train_idx])\n",
    "            oof_preds[val_idx] = clf.predict_proba(X[val_idx])[:, 1]\n",
    "            \n",
    "            del clf\n",
    "            gc.collect()\n",
    "        \n",
    "        # Threshold tuning\n",
    "        best_f1, best_thresh = 0.0, 0.5\n",
    "        \n",
    "        for thresh in np.linspace(0.1, 0.9, 17):\n",
    "            y_pred = (oof_preds >= thresh).astype(int)\n",
    "            f1 = f1_score(y, y_pred, zero_division=0)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thresh = thresh\n",
    "        \n",
    "        print(f\"-> F1: {best_f1:.4f} @ thresh={best_thresh:.3f}\")\n",
    "        \n",
    "        # Train final model\n",
    "        final_clf = LGBMClassifier(\n",
    "            n_estimators=100, learning_rate=0.05, max_depth=5,\n",
    "            num_leaves=31, subsample=0.8, colsample_bytree=0.8,\n",
    "            min_child_sample=20, reg_alpha=0.1, reg_lambda=0.1, force_col_wise=True,\n",
    "            random_state=42, n_jobs=-1, verbose=-1\n",
    "        )\n",
    "        final_clf.fit(X, y)\n",
    "        \n",
    "        # Save model và threshold\n",
    "        joblib.dump(final_clf, f\"{save_dir}/{action}_model.pkl\")\n",
    "        joblib.dump(best_thresh, f\"{save_dir}/{action}_threshold.pkl\")\n",
    "        \n",
    "        thresholds[action] = best_thresh\n",
    "        f1_scores.append((section_idx, mode, action, best_f1))\n",
    "        \n",
    "        del final_clf, oof_preds\n",
    "        gc.collect()\n",
    "    \n",
    "    return f1_scores, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841682fe",
   "metadata": {
    "papermill": {
     "duration": 0.003356,
     "end_time": "2025-12-01T00:34:27.996141",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.992785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15961c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:28.004027Z",
     "iopub.status.busy": "2025-12-01T00:34:28.003829Z",
     "iopub.status.idle": "2025-12-01T00:34:28.014542Z",
     "shell.execute_reply": "2025-12-01T00:34:28.014040Z"
    },
    "papermill": {
     "duration": 0.016177,
     "end_time": "2025-12-01T00:34:28.015706",
     "exception": false,
     "start_time": "2025-12-01T00:34:27.999529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_actions(X, models, actions):\n",
    "    \"\"\"\n",
    "    Dự đoán nhiều action.\n",
    "    \n",
    "    Args:\n",
    "        X: feature DataFrame\n",
    "        models: dict of trained classifiers {action: model}\n",
    "        actions: danh sách actions để predict\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame với xác suất mỗi actions\n",
    "    \"\"\"\n",
    "    \n",
    "    proba_df = pd.DataFrame(index=X.index)\n",
    "    \n",
    "    for action in actions:\n",
    "        if action not in models:\n",
    "            # Action not trained in this section\n",
    "            proba_df[action] = 0.0\n",
    "            continue\n",
    "        \n",
    "        model = models[action]\n",
    "        \n",
    "        try:\n",
    "            proba = model.predict_proba(X.values)[:, 1]\n",
    "        except (ValueError, AttributeError) as e:\n",
    "            # Feature mismatch or missing feature_names_in_\n",
    "            # Align features to match what model expects\n",
    "            try:\n",
    "                # Get features model was trained on\n",
    "                expected_features = model.feature_names_in_\n",
    "                \n",
    "                # Reindex X to match expected features\n",
    "                # Missing features filled with 0\n",
    "                X_aligned = X.reindex(columns=expected_features, fill_value=0)\n",
    "                proba = model.predict_proba(X_aligned.values)[:, 1]\n",
    "                \n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    n_features = model.n_features_in_\n",
    "                    \n",
    "                    if X.shape[1] < n_features:\n",
    "                        # Pad with zeros\n",
    "                        padding = pd.DataFrame(\n",
    "                            0, \n",
    "                            index=X.index, \n",
    "                            columns=range(X.shape[1], n_features)\n",
    "                        )\n",
    "                        X_padded = pd.concat([X, padding], axis=1)\n",
    "                        proba = model.predict_proba(X_padded.values)[:, 1]\n",
    "                    elif X.shape[1] > n_features:\n",
    "                        # Truncate\n",
    "                        proba = model.predict_proba(X.iloc[:, :n_features].values)[:, 1]\n",
    "                    else:\n",
    "                        proba = model.predict_proba(X.values)[:, 1]\n",
    "                        \n",
    "                except:\n",
    "                    # Last resort: skip this action\n",
    "                    print(f\"⚠ Could not predict {action}, skipping\")\n",
    "                    proba_df[action] = 0.0\n",
    "                    continue\n",
    "        \n",
    "        proba_df[action] = proba\n",
    "    \n",
    "    return proba_df\n",
    "\n",
    "\n",
    "def predict_multiclass(pred, meta, thresholds):\n",
    "    \"\"\"\n",
    "    Đổi frame probabilities thành chuỗi frame có action đó\n",
    "    Args:\n",
    "        pred: DataFrame (num_frames, num_actions) with probabilities\n",
    "        meta: DataFrame with video_frame, agent_id, target_id\n",
    "        thresholds: dict of thresholds per action\n",
    "    Returns:\n",
    "        DataFrame with columns: video_id, agent_id, target_id, action, start_frame, stop_frame\n",
    "    \"\"\"\n",
    "    if len(pred.columns) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Get action có xác suất cao nhất mỗi frame\n",
    "    ama = np.argmax(pred.values, axis=1)\n",
    "    max_proba = pred.max(axis=1).values\n",
    "    \n",
    "    # Áp dụng thresholds\n",
    "    threshold_array = np.array([thresholds.get(col, 0.5) for col in pred.columns])\n",
    "    action_thresholds = threshold_array[ama]\n",
    "    ama = np.where(max_proba >= action_thresholds, ama, -1)\n",
    "    ama = pd.Series(ama, index=meta.video_frame.values)\n",
    "    \n",
    "    # Detect changes\n",
    "    changes_mask = (ama != ama.shift(1)).values\n",
    "    ama_changes = ama[changes_mask]\n",
    "    meta_changes = meta[changes_mask]\n",
    "    \n",
    "    mask = ama_changes.values >= 0\n",
    "    mask[-1] = False\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    submission_part = pd.DataFrame({\n",
    "        'video_id': meta_changes[\"video_id\"].values[mask],\n",
    "        'agent_id': meta_changes[\"agent_id\"].values[mask],\n",
    "        'target_id': meta_changes[\"target_id\"].values[mask],\n",
    "        'action': pred.columns[ama_changes.values[mask]],\n",
    "        'start_frame': ama_changes.index[mask],\n",
    "        'stop_frame': ama_changes.index[1:][mask[:-1]]\n",
    "    })\n",
    "    \n",
    "    # Fix stop_frame với mỗi bộ video/agent/target\n",
    "    stop_video_id = meta_changes[\"video_id\"].values[1:][mask[:-1]]\n",
    "    stop_agent_id = meta_changes[\"agent_id\"].values[1:][mask[:-1]]\n",
    "    stop_target_id = meta_changes[\"target_id\"].values[1:][mask[:-1]]\n",
    "    \n",
    "    for i in range(len(submission_part)):\n",
    "        video_id = submission_part.video_id.iloc[i]\n",
    "        agent_id = submission_part.agent_id.iloc[i]\n",
    "        target_id = submission_part.target_id.iloc[i]\n",
    "        \n",
    "        if (stop_video_id[i] != video_id or \n",
    "            stop_agent_id[i] != agent_id or \n",
    "            stop_target_id[i] != target_id):\n",
    "\n",
    "            new_stop_frame = meta[meta.video_id == video_id].video_frame.max() + 1\n",
    "            submission_part.iat[i, submission_part.columns.get_loc(\"stop_frame\")] = new_stop_frame\n",
    "    \n",
    "    return submission_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f783acf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:28.023837Z",
     "iopub.status.busy": "2025-12-01T00:34:28.023638Z",
     "iopub.status.idle": "2025-12-01T00:34:28.033056Z",
     "shell.execute_reply": "2025-12-01T00:34:28.032376Z"
    },
    "papermill": {
     "duration": 0.015026,
     "end_time": "2025-12-01T00:34:28.034130",
     "exception": false,
     "start_time": "2025-12-01T00:34:28.019104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_submission(submission, dataset, traintest, traintest_directory=None):\n",
    "    \"\"\"\n",
    "    Làm sạch submission:\n",
    "    1. Bỏ các chuỗi start_frame >= stop_frame\n",
    "    2. Bỏ các chuỗi bị lặp (cùng agent-target)\n",
    "    3. Điền video trống với dummy predictions\n",
    "    \"\"\"\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "    \n",
    "    # Bỏ invalid frames\n",
    "    old_len = len(submission)\n",
    "    submission = submission[submission.start_frame < submission.stop_frame]\n",
    "    if len(submission) != old_len:\n",
    "        print(f\"⚠ Removed {old_len - len(submission)} events with start >= stop\")\n",
    "    \n",
    "    # Bỏ chuỗi bị lặp\n",
    "    old_len = len(submission)\n",
    "    group_list = []\n",
    "    for _, group in submission.groupby([\"video_id\", \"agent_id\", \"target_id\"]):\n",
    "        group = group.sort_values(\"start_frame\")\n",
    "        mask = np.ones(len(group), dtype=bool)\n",
    "        last_stop_frame = 0\n",
    "        for i, (_, row) in enumerate(group.iterrows()):\n",
    "            if row[\"start_frame\"] < last_stop_frame:\n",
    "                mask[i] = False\n",
    "            else:\n",
    "                last_stop_frame = row[\"stop_frame\"]\n",
    "        group_list.append(group[mask])\n",
    "    \n",
    "    submission = pd.concat(group_list)\n",
    "    if len(submission) != old_len:\n",
    "        print(f\"⚠ Removed {old_len - len(submission)} overlapping events\")\n",
    "    \n",
    "    # Điền video trống\n",
    "    s_list = []\n",
    "    for idx, row in dataset.iterrows():\n",
    "        lab_id = row[\"lab_id\"]\n",
    "        if lab_id.startswith('MABe22'):\n",
    "            continue\n",
    "        \n",
    "        video_id = row[\"video_id\"]\n",
    "        if (submission.video_id == video_id).any():\n",
    "            continue\n",
    "\n",
    "        if type(row.behaviors_labeled) != str:\n",
    "            continue\n",
    "        \n",
    "        print(f\"⚠ Video {video_id} has no predictions, filling...\")\n",
    "        \n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        try:\n",
    "            vid = pd.read_parquet(path)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        vid_behaviors = json.loads(row[\"behaviors_labeled\"])\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=[\"agent\", \"target\", \"action\"])\n",
    "        \n",
    "        start_frame = vid.video_frame.min()\n",
    "        stop_frame = vid.video_frame.max() + 1\n",
    "        \n",
    "        for (agent, target), actions in vid_behaviors.groupby([\"agent\", \"target\"]):\n",
    "            batch_length = int(np.ceil((stop_frame - start_frame) / len(actions)))\n",
    "            for i, (_, action_row) in enumerate(actions.iterrows()):\n",
    "                batch_start = start_frame + i * batch_length\n",
    "                batch_stop = min(batch_start + batch_length, stop_frame)\n",
    "                s_list.append((video_id, agent, target, action_row[\"action\"], batch_start, batch_stop))\n",
    "    \n",
    "    if len(s_list) > 0:\n",
    "        submission = pd.concat([\n",
    "            submission,\n",
    "            pd.DataFrame(s_list, columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "        ])\n",
    "    \n",
    "    submission = submission.reset_index(drop=True)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f720f05",
   "metadata": {
    "papermill": {
     "duration": 0.003332,
     "end_time": "2025-12-01T00:34:28.040932",
     "exception": false,
     "start_time": "2025-12-01T00:34:28.037600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e1745",
   "metadata": {
    "papermill": {
     "duration": 0.00334,
     "end_time": "2025-12-01T00:34:28.047591",
     "exception": false,
     "start_time": "2025-12-01T00:34:28.044251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702a552e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:28.055581Z",
     "iopub.status.busy": "2025-12-01T00:34:28.055314Z",
     "iopub.status.idle": "2025-12-01T00:34:28.066547Z",
     "shell.execute_reply": "2025-12-01T00:34:28.066012Z"
    },
    "papermill": {
     "duration": 0.016623,
     "end_time": "2025-12-01T00:34:28.067545",
     "exception": false,
     "start_time": "2025-12-01T00:34:28.050922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == \"validate\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    thresholds_all = {\"single\": {}, \"pair\": {}}\n",
    "    f1_list_all = []\n",
    "\n",
    "    # Train model\n",
    "    for section in range(len(body_parts_list)):\n",
    "        # Lấy body_parts_tracked trong số 9 bộ của toàn dataset\n",
    "        body_parts_tracked_str = body_parts_list[section]\n",
    "        body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "\n",
    "        if len(body_parts_tracked) > 5:\n",
    "            body_parts_tracked = [b for b in body_parts_tracked if b not in CFG.drop_body_parts]\n",
    "\n",
    "        # Lấy các rows/videos được thu với body_parts_tracked tương ứng\n",
    "        train_subset = train[train.body_parts_tracked == body_parts_tracked_str]\n",
    "\n",
    "        if train_subset.empty:\n",
    "            print(\"\\nNo videos in this section, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"SECTION {section}/{len(body_parts_list)-1} (9 sections total): {len(body_parts_tracked)} bodyparts, {len(train_subset)} videos\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        fps_lookup = (\n",
    "            train_subset[[\"video_id\", \"frames_per_second\"]]\n",
    "            .drop_duplicates(\"video_id\")\n",
    "            .set_index(\"video_id\")[\"frames_per_second\"]\n",
    "            .to_dict()\n",
    "        )\n",
    "    \n",
    "        single_mouse = []\n",
    "        single_meta = []\n",
    "        single_y = []\n",
    "\n",
    "        pair_mouse = []\n",
    "        pair_meta = []\n",
    "        pair_y = []\n",
    "\n",
    "        # Accumulate generated data\n",
    "        for mode, data, meta, labels in generate_mouse_data(train_subset, mode=\"train\"):\n",
    "            video_id = meta[\"video_id\"].iloc[0]\n",
    "            fps = fps_lookup.get(video_id, 30.0)\n",
    "\n",
    "            if mode == \"single\":\n",
    "                single_mouse.append(data)\n",
    "                single_meta.append(meta)\n",
    "                single_y.append(labels)\n",
    "\n",
    "            else:\n",
    "                pair_mouse.append(data)\n",
    "                pair_meta.append(meta)\n",
    "                pair_y.append(labels)\n",
    "\n",
    "            del data, meta, labels\n",
    "        gc.collect()\n",
    "\n",
    "        # Train single models\n",
    "        if len(single_mouse) > 0:\n",
    "            single_X = []\n",
    "\n",
    "            for data_i, meta_i, in zip(single_mouse, single_meta):\n",
    "                fps_i = _fps_from_meta(meta_i, fps_lookup, default_fps=30.0)\n",
    "\n",
    "                X_i = build_single_features(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                single_X.append(X_i)\n",
    "\n",
    "                del X_i, fps_i\n",
    "            gc.collect()\n",
    "        \n",
    "            X_all = pd.concat(single_X, ignore_index=True)\n",
    "            y_all = pd.concat(single_y, ignore_index=True)\n",
    "            meta_all = pd.concat(single_meta, ignore_index=True)\n",
    "\n",
    "            del single_X, single_mouse, single_meta, single_y\n",
    "            gc.collect()\n",
    "        \n",
    "            print(f\"Shape: {X_all.shape[0]} frames × {X_all.shape[1]} features\")\n",
    "        \n",
    "            f1_list, thresholds = train_action_models(\n",
    "                X_all, y_all, meta_all, section, \"single\", n_splits=3\n",
    "            )\n",
    "            f1_list_all.extend(f1_list)\n",
    "            thresholds_all[\"single\"][section] = thresholds\n",
    "\n",
    "            del X_all, y_all, meta_all\n",
    "            gc.collect()\n",
    "\n",
    "        # Train pair models\n",
    "        if len(pair_mouse) > 0:\n",
    "            pair_X = []\n",
    "            for data_i, meta_i in zip(pair_mouse, pair_meta):\n",
    "                fps_i = _fps_from_meta(meta_i, fps_lookup, default_fps=30.0)\n",
    "\n",
    "                X_i = build_pair_features(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                pair_X.append(X_i)   \n",
    "\n",
    "                del X_i, fps_i\n",
    "            gc.collect()\n",
    "\n",
    "            X_all = pd.concat(pair_X, ignore_index=True)\n",
    "            y_all = pd.concat(pair_y, ignore_index=True)\n",
    "            meta_all = pd.concat(pair_meta, ignore_index=True)  \n",
    "\n",
    "            del pair_X, pair_mouse, pair_y, pair_meta\n",
    "            gc.collect()\n",
    "        \n",
    "            print(f\"Shape: {X_all.shape[0]} frames × {X_all.shape[1]} features\")\n",
    "        \n",
    "            f1_list, thresholds = train_action_models(\n",
    "                X_all, y_all, meta_all, section, \"pair\", n_splits=3\n",
    "            )\n",
    "            f1_list_all.extend(f1_list)\n",
    "            thresholds_all[\"pair\"][section] = thresholds\n",
    "\n",
    "            del X_all, y_all, meta_all\n",
    "            gc.collect()\n",
    "\n",
    "    # Save thresholds and F1 scores\n",
    "    joblib.dump(thresholds_all, \"/kaggle/working/thresholds.pkl\")\n",
    "    f1_df = pd.DataFrame(f1_list_all, columns=[\"section\", \"mode\", \"action\", \"f1\"])\n",
    "    joblib.dump(f1_df, \"/kaggle/working/f1_scores.pkl\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Training complete! Mean F1: {f1_df['f1'].mean():.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc966e",
   "metadata": {
    "papermill": {
     "duration": 0.003326,
     "end_time": "2025-12-01T00:34:28.074709",
     "exception": false,
     "start_time": "2025-12-01T00:34:28.071383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Testing pipeline - Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d384e03c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T00:34:28.082762Z",
     "iopub.status.busy": "2025-12-01T00:34:28.082528Z",
     "iopub.status.idle": "2025-12-01T00:35:05.864814Z",
     "shell.execute_reply": "2025-12-01T00:35:05.864057Z"
    },
    "papermill": {
     "duration": 37.790682,
     "end_time": "2025-12-01T00:35:05.868942",
     "exception": false,
     "start_time": "2025-12-01T00:34:28.078260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREDICTION PIPELINE\n",
      "============================================================\n",
      "Processing video 438887472 (section 0)...\n",
      "\n",
      "============================================================\n",
      "SUBMISSION COMPLETE!\n",
      "============================================================\n",
      "Total events: 855\n",
      "Unique videos: 1\n",
      "Actions: {'rear': 593, 'approach': 136, 'avoid': 86, 'submit': 18, 'chase': 18, 'attack': 4}\n",
      "Saved to: submission.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load thresholds\n",
    "thresholds_all = joblib.load(f\"{CFG.model_save_dir}/thresholds.pkl\")\n",
    "submission_list = []\n",
    "\n",
    "# Create fps_lookup for test set\n",
    "fps_lookup = (\n",
    "    test_csv[[\"video_id\", \"frames_per_second\"]]\n",
    "    .drop_duplicates(\"video_id\")\n",
    "    .set_index(\"video_id\")[\"frames_per_second\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "for idx, row in test_csv.iterrows():\n",
    "    video_id = row.video_id\n",
    "    lab_id = row.lab_id\n",
    "    pix_per_cm = row.pix_per_cm_approx\n",
    "    fps = row.frames_per_second\n",
    "    body_parts_tracked_str = row.body_parts_tracked\n",
    "    \n",
    "    # Tìm section\n",
    "    try:\n",
    "        section = body_parts_list.index(body_parts_tracked_str)\n",
    "    except ValueError:\n",
    "        print(f\"Video {video_id}: body_parts not in training, skip\")\n",
    "        continue\n",
    "    \n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "    if len(body_parts_tracked) > 5:\n",
    "        body_parts_tracked = [b for b in body_parts_tracked if b not in CFG.drop_body_parts]\n",
    "    \n",
    "    print(f\"Processing video {video_id} (section {section})...\")\n",
    "\n",
    "    # Tạo mini-dataset chỉ cho video này\n",
    "    video_df = pd.DataFrame([row])\n",
    "\n",
    "    single_dir = f\"{CFG.model_save_dir}/models/{section}/single\"\n",
    "    pair_dir = f\"{CFG.model_save_dir}/models/{section}/pair\"\n",
    "        \n",
    "    # Load single models\n",
    "    models_single = {}\n",
    "    if os.path.exists(single_dir):\n",
    "        for f in os.listdir(single_dir):\n",
    "            if f.endswith(\"_model.pkl\"):\n",
    "                action = f.replace(\"_model.pkl\", \"\")\n",
    "                models_single[action] = joblib.load(f\"{single_dir}/{f}\")\n",
    "        \n",
    "    # Load pair models\n",
    "    models_pair = {}\n",
    "    if os.path.exists(pair_dir):\n",
    "        for f in os.listdir(pair_dir):\n",
    "            if f.endswith(\"_model.pkl\"):\n",
    "                action = f.replace(\"_model.pkl\", \"\")\n",
    "                models_pair[action] = joblib.load(f\"{pair_dir}/{f}\")\n",
    "        \n",
    "    thr_single = thresholds_all[\"single\"].get(section, {})\n",
    "    thr_pair = thresholds_all[\"pair\"].get(section, {})\n",
    "\n",
    "    # Predict single\n",
    "    for mode, data, meta, actions in generate_mouse_data(video_df, mode=\"test\", generate_single=True, generate_pair=False):\n",
    "        fps_i = _fps_from_meta(meta, fps_lookup, default_fps=30.0)\n",
    "\n",
    "        if len(models_single) == 0:\n",
    "            continue\n",
    "        # Feature engineering\n",
    "        X = build_single_features(data, body_parts_tracked, fps_i)\n",
    "\n",
    "        # Predict\n",
    "        preds = predict_actions(X, models_single, actions)\n",
    "\n",
    "        if len(preds.columns) > 0:\n",
    "            sub_part = predict_multiclass(preds, meta, thr_single)\n",
    "            if len(sub_part) > 0:\n",
    "                submission_list.append(sub_part)\n",
    "\n",
    "        del data, X, preds\n",
    "        gc.collect()\n",
    "\n",
    "    # Predict pair\n",
    "    for mode, data, meta, actions in generate_mouse_data(video_df, mode=\"test\", generate_single=False, generate_pair=True):\n",
    "        fps_i = _fps_from_meta(meta, fps_lookup, default_fps=30.0)\n",
    "\n",
    "        if len(models_pair) == 0:\n",
    "            continue\n",
    "        # Feature engineering\n",
    "        X = build_pair_features(data, body_parts_tracked, fps_i)\n",
    "\n",
    "        # Predict\n",
    "        preds = predict_actions(X, models_pair, actions)\n",
    "\n",
    "        if len(preds.columns) > 0:\n",
    "            sub_part = predict_multiclass(preds, meta, thr_pair)\n",
    "            if len(sub_part) > 0:\n",
    "                submission_list.append(sub_part)\n",
    "\n",
    "        del data, X, preds\n",
    "        gc.collect()\n",
    "\n",
    "    del models_single, models_pair\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# Tạo final submission\n",
    "if len(submission_list) > 0:\n",
    "    submission = pd.concat(submission_list, ignore_index=True)\n",
    "else:\n",
    "    # Empty fallback\n",
    "    print(\"WARNING: No predictions generated!\")\n",
    "    submission = pd.DataFrame({\n",
    "        \"video_id\": [], \"agent_id\": [], \"target_id\": [],\n",
    "        \"action\": [], \"start_frame\": [], \"stop_frame\": []\n",
    "    })\n",
    "\n",
    "# Làm sạch submission\n",
    "submission = clean_submission(submission, test_csv, \"test\", CFG.test_tracking_path)\n",
    "    \n",
    "# Thêm row_id\n",
    "submission.insert(0, \"row_id\", range(len(submission)))\n",
    "    \n",
    "# Save\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total events: {len(submission):,}\")\n",
    "print(f\"Unique videos: {submission.video_id.nunique()}\")\n",
    "print(f\"Actions: {submission.action.value_counts().to_dict()}\")\n",
    "print(f\"Saved to: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8870798,
     "sourceId": 13921080,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8882609,
     "sourceId": 13937998,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.665406,
   "end_time": "2025-12-01T00:35:08.362281",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-01T00:34:04.696875",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
